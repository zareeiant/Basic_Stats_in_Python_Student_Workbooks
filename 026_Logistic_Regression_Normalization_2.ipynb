{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: StatsModels and Details of Regression\n",
    "\n",
    "We can do some more logitic regression to make classification predictions, there's some things that we do to try to drive accuracy up, and some other work we can do to interpret the accuracy better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read diabetes data\n",
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Accuracy\n",
    "\n",
    "Before we get going, we'll touch on a simple concept - the baseline accuracy. For example, in post WW2 Germany the ratio of males to females was approximately .6 (3:5) for people in their 20s - or approximately 5/8 or 62.5% of people were females. \n",
    "\n",
    "If we were to build some model to predict if a twenty-soemthing was a male or female, this should be the worst we can do. A 'default' model of always guessing female will be 62.5% accurate, so if we can't beat that, we suck and are useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline for diabetes\n",
    "1 - df[\"Outcome\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... a model guessing NO at all times would be ~65% accurate here, so that's our worst case scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatsModels Formula Based Logistic Regression\n",
    "\n",
    "The book uses the forumla method for calling statsmodels, whereas when we did linear regression, we used the 'regular' way. For logistic regression, we can try using the formula here. As with linear regression, statsmodels gives us some more detailed data that we can look at to try to understand the effectiveness of the model, such as p values for each feature. \n",
    "\n",
    "Like linear regression, scikitlearn vs statsmodels vs any other library function is pretty much up to you. They do the same thing, so use what you please. SKlearn is a little more transferable to future uses in terms of mechanics. \n",
    "\n",
    "With the statsmodels formula, we can save a little bit of data manipulation in exchange for writing out the feature names. Most notably, we can keep the Xs and Y together in the data, then when we write the formula, that does the 'splitting'. \n",
    "\n",
    "<b>Note:</b> like many functions, the train test split is able to handle data in different data structures. Here if we give it a dataframe it will allow us to keep the varaible names all the way through the statsmodels calculations. This is easy in this case because the x/y splitting in statsmodels is defined here by the formula, not the arrays we feed it. To some degree it is up to you exactly how you manage things like this, I personally like to keep data in a dataframe until we need to do the x/y split, then put the data into the two arrays. This will generally work with anything we use, so I don't need to worry about adjusting. You don't have to follow that, so if some other method makes sense, go for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data - we are keeping the DF to make the results nice\n",
    "#If doing a dataframe, there's no x/y split. So we basically cut the function in half\n",
    "train1, test1 = train_test_split(df,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels Formula\n",
    "\n",
    "The formual for statsmodels is relatively simple. The format is:\n",
    "\n",
    "Y ~ x1 + x2 + x3....\n",
    "\n",
    "Then we feed the logit formula the dataframe to use as well as the formula that tells it which columns go where. After that, it is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula\n",
    "form = \"Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age\"\n",
    "model1 = sm.logit(data=train1, formula=form).fit()\n",
    "\n",
    "#Make predictions for later, get summary for now\n",
    "preds1 = model1.predict(test1)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varaible Importance\n",
    "\n",
    "We can also use the statsmodels results to get an evaluation of varaible importance. The p-value is a measure of the variable importance, and it works much like p-values do in general - if it is small, typically <.05, then the relationship is significant. This can help guide us in looking at which features are important in making a prediction and which are not. \n",
    "\n",
    "One note for this, and any other varaibel selection things that we look at is that they are not definitive and conclusive findings, they are indications. For this linear regression model we can see that right now the SkinThickness, Insulin, and DPG may be candidates for being the least significant contributors. This does not automatically translate to just removing everything with a p-value < .05 and calling it a day. We'll look at some consdierations for selecting varaibles as we work through the ML stuff, but some things that we've looked at that might complicate this are:\n",
    "<ul>\n",
    "<li> The explained varaince - the R2. Recall that our goal is to capture as much of the information that defines how the target varies as possible. Suppose we have a model with 5 features - 4 of them providing the predictive value and one less important. Think of a model to predict how good a teacher is - the target might be \"test results\", and the features might be \"hours creating course material\", \"years of experience\", \"knowledge of material\", \"quality of assignments\", and \"color of whiteboard marker used\". In this example, the color of whiteboard marker can make a difference, I can definitely recall classes where it was hard to read the board, especially if you need glasses. However, this is only a small factor. If this model exists, the whiteboard marker color would probably have a small p-value, as it is not that impactful; however, it stil has some importance, and adds some value, just much less than the other ones. Removing this doesn't make our model better at predicting, it just makes it more compact - we would have to balance between those two. \n",
    "<li> In general, unless a variable is \"bad\" (confounding data), removing it probably won't make accuracy leap. \n",
    "<li> Different models will react differently. These linear models capture linear relationships, other models can capture different relationships, so if the feature set changes, there might be totally different impacts on different models. This is generally not something we can predict all that accurately in advance. \n",
    "</ul>\n",
    "\n",
    "Feature selection is a big topic and we'll revist it as we go. For now, we can use our tool of the p-value to rank variables in terms of their importance, and remove ones in that order if needed. \n",
    "\n",
    "<b>Note on collinearity:</b> if we flash back to collinearity we saw that sometimes we have variables which all contribute the same information to the model. If that is the case, this p-value for importance is somewhat random as the model can't accurately attribute importance to each varaible, since each is offering the same information there's no way to split that. For example, if a model predicted pant size, based on thigh size, knee size, and calf size,  the p-values between those 3 features would be unreliable. Which one is important in that prediction? We don't know, so which one has a higher or lower p-value doesn't matter. If we cut the collinear values first, then there is only one varaible measuring \"leg size\", so any variation of pant size predictions due to the leg size, is due to the one variable that remains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the head of the predictions. \n",
    "preds1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions give us the probabilities, we need to convert to 0-1 to give 'real' answers. We'll make a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prob_to_label(prob, cutoff = 0.5):\n",
    "    label = []\n",
    "    for i in range(len(prob)):\n",
    "        if prob[i] > cutoff:\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = convert_prob_to_label(np.array(preds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test1[\"Outcome\"], labels)\n",
    "sns.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create calc for misclassification rate. \n",
    "#We can look this up, but it is just adding up the errors and dividing. \n",
    "#The confusion matrix is an array, so we just need to grab the correct cells. \n",
    "#The is the compliment to the accuracy score, so we can just use library functions in general\n",
    "mis_rate = (conf_matrix[[1],[0]].flat[0] + conf_matrix[[0],[1]].flat[0])/len(test1)\n",
    "print(mis_rate)\n",
    "print(accuracy_score(test1[\"Outcome\"], labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy Metrics\n",
    "\n",
    "Now, the accuracy is ok, but we have a noticable imbalance between false negatives and false positives (exact split we bounce around due to randomness - when I built this the frist run was pretty large, other runs were smaller). We previously looked briefly at all the different accuracy metrics that can be generated from the confusion matrix. In real use, you need to combine some domain knowledge with these results to choose the measures that make sense for what you're doing. \n",
    "\n",
    "<b>Note:</b> this one shows normalized values, aka percentages. \n",
    "\n",
    "![Confusion Matrix](images/conf_mat_small.png \"Confusion Matrix\")\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision is the ratio of the True Positives to all of the True Positives + False Positives. \n",
    "\n",
    "Precision is most useful as a metric when we want to avoid false positives - think about if you are predicting terrorists, you don't want to Guantanamo aunt Betty on the way home from her bake sale. \n",
    "\n",
    "### Recall / Sensitivity\n",
    "\n",
    "Recall is the ratio of the True Positives to the True Positives + False Negatives. \n",
    "\n",
    "Recall is most useful as a metric when we really want to identify all of the positives - think about if you are predicting terrorists, you want to get them all. \n",
    "\n",
    "![Precision - Recall](images/prec_rec.png \"Precision - Recall\")\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "There is one score that is pretty common that may be helpful  - the F1 score. The F1 score seeks to balance PRECISION (avoid false positives) and RECALL (target true positives, at cost of false positives). There's also a way (fbeta) to adjust the balance between recall and precision, but we'll set that aside for now. The f1 score is:\n",
    "\n",
    "$ F1 = 2 * \\frac{(precision * recall)}{(precision + recall)} $\n",
    "\n",
    "In practice, it is simple with an sklearn function. \n",
    "\n",
    "### Which One?\n",
    "\n",
    "Using a metric other than accuracy will ultimately be defined by the problem being predicted. If there's no outside pressure, the overall accuracy and the F1 will probably be what we normally look at. \n",
    "\n",
    "### Side Note: Other Metrics\n",
    "\n",
    "Below the F1, I put in log-loss, which is another metric of accuracy. This one is commonly seen later on when doing neural network stuff. The idea is always the same - we want to find something that minimizes the amount of error. In a question on the credit card fraud I said that you may try to optimize for some other metric than raw accuracy - this is an example of one that is common. In short, you'll define a ML algorithm to use, then define a loss function (e.g. log-loss), and the algorithm will repeat trials (gradient descent) in an effort to minimize that \"loss\". \n",
    "\n",
    "Log Loss is the negative average of the log of corrected predicted probabilities for each instance. E.g. if a value is true, and a prediction is .8 probability, the corrected probability is .8; if a value is false and the prediction is .8 probability, the corrected proability is .2. These values are \"logged\", then the negative average is taken (the logs are negative), and that's the metric for loss - or how bad the predictions are. Less loss, more accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also use the accuracy score for sklearn. \n",
    "print(\"Acc%:\", accuracy_score(test1[\"Outcome\"], labels))\n",
    "print(\"F1:\", f1_score(test1[\"Outcome\"], labels))\n",
    "print(\"LogLoss:\", log_loss(test1[\"Outcome\"], preds1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, roughly 77% accuracy. Put that in our pockets. We are clearly better than the baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "Feature scaling includes several similar methods of taking numerical values and transforming them to be on a different scale. There are several, a few we'll look at right now are:\n",
    "<ul>\n",
    "<li>Normalization - rescale the data so all values are between 0 and 1. \n",
    "<li>Standardization - rescale the data so there is a mean of 0 and a standard deviation of 1. \n",
    "</ul>\n",
    "\n",
    "These all process our data in a similar way - taking the original data, and shifting its distribution using a transformation. The calculations for these scaling methods are:\n",
    "<ul>\n",
    "<li>Normalization: x = ( (x-min(x)) / (max(x)-min(x)) ) \n",
    "<li>Standardization: x = ( (x-mean(x)) / (std(x)) )\n",
    "</ul>\n",
    "\n",
    "![Scaling](images/scaling.png \"Scaling\")\n",
    "\n",
    "Note: There are other feature scaling algorithms/methods, these are just the two more common ones. The idea is pretty much always the same. One other consdieration is how the scaling treats outliers, which we'll worry about more next semester. \n",
    "\n",
    "![Standardization - Normalization](images/norm_stand.jpeg \"Standardization - Normalization\")\n",
    "\n",
    "<h3>OK.... Why?</h3>\n",
    "\n",
    "Feature scaling has several benefits, the impact of these benefits varies widely depending on the exact original data, and the type of models you're using. The reasons are:\n",
    "<ul>\n",
    "<li>Scaling - different values may have widely different scales (e.g. if processing a loan, age and net worth will be very different). Scaling can sometimes cause problems with the relative impact of different ranges distorting calculations. Small values can be \"drowned out\" by larger values. \n",
    "<li>Range - similar to the scale problem, if data values are radically different, some calculations will become less accurate. This particularly can impact distance based calculations, like clustering. \n",
    "<li>Speed - with algorithms that use methods like gradient descent (like logistic regression), having values on different scales and ranges may cause the algorithm to take longer to converge on a solution, or potentially prevent it at all. \n",
    "</ul>\n",
    "\n",
    "Feature scaling can improve predictive accuracy, sometimes dramatically. Scaling data is not super impactful in some calculations (linear regression, trees) and it can (can - not will) be very impactful in others (logistic regression, gradient descent, neural networks, PCA). In general, when we have an algorithm that is adjusting itself to try to find the most accurate solution (e.g. log. reg. with gradient descent), scaling the data helps and is pretty standard. When we get to things like neural networks later on, it is just part of the process. \n",
    "\n",
    "We will do this pretty often in machine learning applications, similar to encoding categorical varaibles it is kind of a preparatory step that we just do without thinking much about it because it makes things work (or work better).\n",
    "\n",
    "<h3>Which Scaler to Pick?</h3>\n",
    "\n",
    "There is not generally a definitive answer to that, and the real answer is to try a few and observe the results in accuracy. We do have a few rules of thumb:\n",
    "<ul>\n",
    "<li>Normalization: distribution is unkonwn, things need to be 0 to 1. \n",
    "<li>Standardization: distributions are normal(ish). \n",
    "</ul>\n",
    "\n",
    "For now deciding between the methods isn't a huge concern. If the features look normal, we'll try to standardize; if not, normalize. We can worry about some finer differences as they come up next semester. If in doubt, try each, check accuracy, choose the best. Outliers will impact each (think about why), so we probably want to deal with those prior to scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, we can build a function for each, since they are simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumbNormalizer(x):\n",
    "    x_ = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some example data\n",
    "#d1 = [1,2,3,4,5,9,5,12,7,8,12,5,6,8,2,8,9]\n",
    "#d2 = [1,2,3,4,5,9,5,12,7,12,5,6,8,2,8,9,800]\n",
    "d1 = df[\"BMI\"].to_list()\n",
    "d2 = df[\"Glucose\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize some data\n",
    "tmp = dumbNormalizer(d1)\n",
    "print(np.mean(tmp))\n",
    "print(tmp[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a dumb standardizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumbStandardizer(x):\n",
    "    x_ = (x - np.mean(x)) / (np.std(x))\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize some data\n",
    "tmp = dumbStandardizer(d1)\n",
    "print(np.mean(tmp))\n",
    "print(tmp[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Does the Distribution Change?\n",
    "\n",
    "We can visualize both datasets in their original form, after normalization, and after standardization. The general pattern of the data isn't changed, but the range that it is distributed over is different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize all\n",
    "thinkplot.PrePlot(6,2,3)\n",
    "sns.kdeplot(d1)\n",
    "thinkplot.SubPlot(2)\n",
    "sns.kdeplot(dumbNormalizer(d1))\n",
    "thinkplot.SubPlot(3)\n",
    "sns.kdeplot(dumbStandardizer(d1))\n",
    "thinkplot.SubPlot(4)\n",
    "sns.kdeplot(d2)\n",
    "thinkplot.SubPlot(5)\n",
    "sns.kdeplot(dumbNormalizer(d2))\n",
    "thinkplot.SubPlot(6)\n",
    "sns.kdeplot(dumbStandardizer(d2))\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Outcomes\n",
    "\n",
    "For each of the scaling, the distribution is the same for the data after the transformation, but the ranges are different. We can also see visually the impact of outliers here - we'd normally want to address those in advance. In extreme cases, having a massive outlier can \"squish\" all the data at one end of the distribution, which will be bad in most cases. \n",
    "\n",
    "#### Scaling Caveat\n",
    "\n",
    "One detail we've overlooked here is that the data should technically be scaler after the split of train/test data, and the scaler should only be trained on the training data (fit) and applied on the testing data (transform). This is to prevent any data leakage - the test data is supposed to be brand new, and if it is able to influence the scaling, then it has some impact on the training of the model. How much of an impact does this make? Probabaly not much for the vast majority of applications, but it is technically correct to keep 'em separated. \n",
    "\n",
    "We'll do the below example with it scaled 'properly'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Library Scaling Functions</h3>\n",
    "\n",
    "Luckily, we don't really need to bust out our algebra to build any of these calculations by hand, we can use some built in functions in scikit learn to do it for us. These functions can also be built into a pipeline to process data (next semester), so we can build this transformation in without really seeing the resutlts. Like encoding last week, this takes data that is readable, and makes it not readable - we can package all of those steps in with our modelling, so legible data goes in - a trained model comes out. \n",
    "\n",
    "In scikit learn, the different functions for these methods are (examples below in code):\n",
    "<ul>\n",
    "<li>Normalization: MinMaxScaler\n",
    "<li>Standardization: StandardScaler\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example with Real Functions</h3>\n",
    "\n",
    "Building scaling functions is pretty easy, in practice though we can use the sklearn ones, lets build that into the logistic regression for the diabetes prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make arrays from data\n",
    "dfY = df[\"Outcome\"]\n",
    "dfX = df.drop(columns={\"Outcome\"}) #There isn't generally a need to standardize the Y values\n",
    "\n",
    "x = np.array(dfX)\n",
    "y = np.array(dfY).reshape(-1,1)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create scaler and choose method. \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Comment one out, use the other\n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize or normalize\n",
    "#Use training data to fit the scaler, then apply that predefined scale to the test data\n",
    "x2 = scaler.fit_transform(X_train2)\n",
    "X_trans2 = scaler.transform(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have standardized and normalized data, do linear regression stuff...\n",
    "\n",
    "I'm going to set it up so we can swap the x data in the first line of code below, mostly to save typing. We can run everything twice, once for each scaling technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "model2 = LogisticRegression().fit(x2,y_train2.ravel())\n",
    "\n",
    "#Make predictions\n",
    "preds2 = model2.predict(X_trans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 1/0 results and show results\n",
    "labels2 = convert_prob_to_label(np.array(preds2))\n",
    "conf_matrix2 = confusion_matrix(y_test2,labels2)\n",
    "sns.heatmap(conf_matrix2, annot=True)\n",
    "\n",
    "print(\"F1:\",f1_score(y_test2, labels2))\n",
    "print(\"Acc:\",accuracy_score(y_test2, labels2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay. We are super happy. \n",
    "\n",
    "IIRC, the accuracy without doing any scaling was similar (~76%), that's ok. This data didn't really have any massively differing ranges for the original data. We also didn't do any outlier filtering - what if we try that and repeat? We know from doing the dumb scaling, that there are a few outliers. \n",
    "\n",
    "There's a simple filter commented out up above, we'll go remove and repeat. It might make it better, it might not. This case only has a few outliers and they are not outrageous, so we shouldn't be surprised if the difference is relatively small. We can see that the errors are more balanced, so the F1 score does show improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With a Pipeline\n",
    "\n",
    "We can use our pipeline to string the steps in our data prep together. In general, most of the sklearn functions that we may need can fit into these pipelines. Once the pipeline is made, it \"is\" our model - it will do whatever preparation steps we build into the list of actions. Here, we can use it to string together the scaling and the model; in other models we might add a step to deal with missing data (impute), perform encoding (like the one-hot get_dummies), or any number of steps. This allows us to build what is effectively a new model that contains both all of the data preparation and the modeling, all in one package. This is not required, but tends to make things more manageable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pipeline\n",
    "m2 = make_pipeline( MinMaxScaler(),\n",
    "                    LogisticRegression()\n",
    "                    )\n",
    "\n",
    "# Use pipeline like it is a model. \n",
    "m2.fit(X_train2, y_train2.ravel())\n",
    "m2_pred = m2.predict(X_test2)\n",
    "\n",
    "m2_label = convert_prob_to_label(np.array(m2_pred))\n",
    "m2_matrix = confusion_matrix(y_test2,m2_label)\n",
    "sns.heatmap(m2_matrix, annot=True)\n",
    "\n",
    "print(\"F1:\",f1_score(y_test2, m2_label))\n",
    "print(\"Acc:\",accuracy_score(y_test2, m2_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And With StatsModels - Variable Importance\n",
    "\n",
    "We can also look at the variable importance, and compare any changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_sm, test2_sm = train_test_split(df,test_size=0.3)\n",
    "\n",
    "#Define formula\n",
    "form = \"Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age\"\n",
    "model2_sm = sm.logit(data=train2_sm, formula=form).fit()\n",
    "\n",
    "#Make predictions for later, get summary for now\n",
    "preds2_sm = model2_sm.predict(test2_sm)\n",
    "model2_sm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the original..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The degree of changes here can vary quite a bit, we generally expect more of an impact the more different the ranges are of the varaibles. For example, if you had some model predicting space stuff for a telescope, and some features (like distances) are massive (like hundreds of millions of km), and other features are tiny (like fractions of degrees for angles), that difference would probably cause the model with non-normalized data to be far worse. \n",
    "\n",
    "![Space](images/space.png \"Space\")\n",
    "\n",
    "As with most things, this will impact different models differently. For things that are tree based normalization (normally) makes no difference, for reasons that will see when we look at how trees function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run With Outliers Removed\n",
    "\n",
    "We can also get rid of some outliers and scale the data and see what we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kill outliers. \n",
    "\n",
    "df_o = df[df[\"Glucose\"] > 20]\n",
    "df_o = df_o[df_o[\"BloodPressure\"] > 20]\n",
    "df_o = df_o[df_o[\"SkinThickness\"] < 80]\n",
    "df_o = df_o[df_o[\"Glucose\"] > 20]\n",
    "df_o = df_o[df_o[\"BMI\"] > 10]\n",
    "#df_o.head()\n",
    "\n",
    "#Split and Scale\n",
    "train2_o, test2_o = train_test_split(df_o,test_size=0.3)\n",
    "scaler_o = MinMaxScaler()\n",
    "x_o = scaler_o.fit_transform(train2_o)\n",
    "x_test_o = scaler_o.transform(test2_o)\n",
    "\n",
    "#Make df, so the variable names stay\n",
    "x_o = pd.DataFrame(x_o, columns = df.columns)\n",
    "x_test_o = pd.DataFrame(x_test_o, columns=df.columns)\n",
    "#x_o.head()\n",
    "\n",
    "#Define formula\n",
    "form = \"Outcome ~ Pregnancies + Glucose + BloodPressure + SkinThickness + Insulin + BMI + DiabetesPedigreeFunction + Age\"\n",
    "model_o = sm.logit(data=x_o, formula=form).fit()\n",
    "\n",
    "#Make predictions for later, get summary for now\n",
    "preds_o = model_o.predict(x_test_o)\n",
    "model_o.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_o = convert_prob_to_label(np.array(preds_o))\n",
    "matrix_o = confusion_matrix(test2_o[\"Outcome\"],label_o)\n",
    "sns.heatmap(matrix_o, annot=True)\n",
    "\n",
    "print(\"F1:\",f1_score(test2_o[\"Outcome\"],label_o))\n",
    "print(\"Acc:\",accuracy_score(test2_o[\"Outcome\"],label_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If We Try Removing Some Variables\n",
    "\n",
    "We can drop a few bad p-value features and see what we get. Remember, we don't really expect accuracy to jump, but we suspect that the accuracy won't drop that much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define formula\n",
    "form1 = \"Outcome ~ Pregnancies + Glucose + Insulin + BMI + DiabetesPedigreeFunction\"\n",
    "model_o2 = sm.logit(data=x_o, formula=form1).fit()\n",
    "\n",
    "#Make predictions for later, get summary for now\n",
    "preds_o2 = model_o2.predict(x_test_o)\n",
    "model_o2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_o2 = convert_prob_to_label(np.array(preds_o2))\n",
    "matrix_o2 = confusion_matrix(test2_o[\"Outcome\"],label_o2)\n",
    "sns.heatmap(matrix_o2, annot=True)\n",
    "\n",
    "print(\"F1:\",f1_score(test2_o[\"Outcome\"],label_o2))\n",
    "print(\"Acc:\",accuracy_score(test2_o[\"Outcome\"],label_o2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Normalization Exercise\n",
    "\n",
    "Logistic regression with normalization. \n",
    "\n",
    "Identify penguin sex. In doing so:\n",
    "<ul>\n",
    "<li>Explore the data. \n",
    "<li>Clean any erroneous data. \n",
    "<li>Create classification model. \n",
    "</ul>\n",
    "\n",
    "<b>Note:</b> There are some sections in here because the example solution is setup with some EDA throughout. Follow them if you please, it is not required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sns.load_dataset(\"penguins\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical things look pretty OK. What about categorical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing looks too odd in the exploration (this is a toy dataset, so that's normal). We can do some modelling and predict sex...\n",
    "\n",
    "I will sklearn, because I like it more. Statsmodels imight be useful in a bit, we could throw it in to get p values for the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline is roughly 50/50, how'd we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing pretty well. What could be better? \n",
    "\n",
    "One suspicion I have would be that the species are somewhat different. Gentoo in particular seems to have different metrics, so maybe we'd predict it separately? I might need more data for that. \n",
    "\n",
    "Also, can we improve by removing confounding vars? Look at StatsModels to have an idea of variable relevance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll remove island and flipper length, try again, and see what's up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!\n",
    "\n",
    "Note: We have relatively little data, so the swings for accuracy between trials can be pretty pronounced. The first time I ran this there was a big improvement in accuracy after removing stuff, other runs had a smaller difference. More data would make it more stable, repeating the trials would also. You could build a loop to repeat trials, we'll do it the sklearn way early next semester. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
