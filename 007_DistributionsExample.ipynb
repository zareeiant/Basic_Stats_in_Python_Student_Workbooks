{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinkplot\n",
    "import thinkstats2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes Prior to Starting this Document\n",
    "\n",
    "There are a few things in here that we will fill in the details for later, including:\n",
    "<ul>\n",
    "<li> Extrapolation. \n",
    "<li> Time based datasets (time series)\n",
    "<li> Smoother pmf/histogram plots. \n",
    "<li> <b>Using</b> models created from our data. \n",
    "</ul>\n",
    "\n",
    "Here we are mainly looking to see an exponential distribution in the format it will normally be seen in, and translate that real world scenario to a statistical model. \n",
    "\n",
    "Also, the  textbook examples show some data that is more tightly fitted to the theoretical models. These examples are intended to illustrate with real data, which sometimes doesn't fit; personally, I think dealing with the issues that may arrise with imperfect real data is good preparation in handling weird things that pop up. We <b>can</b> find or generate data that is a perfect or near perfect match to our theoretical distributions, for some people the direct match may be more illustrative; if you want to see examples like this, let me know, we can do them; more simple examples may be able to be made on the fly, more complex ones require a heads up. \n",
    "\n",
    "# Distributions\n",
    "\n",
    "We looked at histograms, pmf, and cdf charts to get an idea of the distribution of our data - or how it is spread out over the range. Are most values clustered together closely? Are values fairly evenly spread out? Are there patterns? Outliers? Clusters?\n",
    "\n",
    "Everything we've dealt with thus far has been emperical - i.e. we've taken actual data, and looked at the shape of that data to see the (actual, real life) distribution. This inheirently depends on collecting some data, and looking at it - this real world aspect has some limitations, you need to have actual data collected.  \n",
    "\n",
    "Now we're going to look at analytical distributions - mathmatical models that represent a distribution. Analytical distributions allow us to look at/analyze/calculate things without having to collect all that data - we can use the mathmatical formula as a siplified representation of what we would expect to find if we did collect the data.\n",
    "\n",
    "In many cases the model can pretty accurately represent the real data, and we can use the simple model to do analysis and make predictions, having faith that the data will closely mirror our calculations.\n",
    "\n",
    "The most common and well know example of this the the normal distribution or bell curve. We can use this model to analyze may distributions we see in real life - the heights, home prices, etc from the demonstrations tend to look a lot like bell curves, as do many things in real life. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal for this one is to get a model that models (part of) COVID growth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load some Covid Data\n",
    "#Note the sep thing - that tells Pandas that the separator is a semi-colon. This is just weird data.\n",
    "#In theory, the separator could be anything (it is usually a comma), just use this if it is\n",
    "df = pd.read_csv(\"data/full_data_2.csv\", sep=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm going to add a \"Day Number\" value. This just makes it easier than using dates.\n",
    "#Note: you could write a function to translate days to dates...\n",
    "#The command to do this is just a Google result \"dataframe add running number\". I add 1 to avoid a log(0) scenario. \n",
    "df[\"Day\"] = np.arange(len(df))\n",
    "df[\"Day\"] = df[\"Day\"].apply(lambda x: x+ 1)\n",
    "df.drop([52,53], inplace=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the growth!\n",
    "sns.lineplot(data=df, x=\"Day\", y=\"Infections\")\n",
    "\n",
    "plt.title('COVID Gonna Give It To Ya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With points!\n",
    "sns.lineplot(data=df, x=\"Day\", y=\"Infections\", style=1, markers=True)\n",
    "plt.title('COVID Gonna Give It To Ya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDF to See Distribution\n",
    "\n",
    "This certainly looks like exponential growth. \n",
    "\n",
    "What we are looking at there ISN'T a distribution though, this is a scatter plot (we'll look at this soon) that shows two variables, time and infections. Distributions show one variable at a time, in this case, just infections. We can focus our attention to the infections data. \n",
    "\n",
    "<b>For exponential distributions this type of thing is common, we are so used to look at them mapped against time, that we don't consider time as a separate variable. In the normal stuff we didn't have to do this separation as each variable is \"more separate\" as compared to the time/count graph we normally get for time.</b>\n",
    "\n",
    "We can make a CDF of our exponential distribution, and see if it matches our expectations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_cdf = thinkstats2.Cdf(df[\"Infections\"])\n",
    "thinkplot.Cdf(inf_cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with the sns version. The ecdf plot is a cummulative distribution plot from seaborn, to find it I Googled \"seaborn cummulative distribution\" and the documentation page was the first link. There are examples near the bottom that I could adapt. Copy and adjust - borrowing and tweaking code that is similar to what you want is good, efficient if you have semi-repetitive or standardized tasks (like data analysis in places), aren't able to code things off the top of your head, or are learning to tackle new problems. (Near) plagarism is desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.ecdfplot(data=df[\"Infections\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at an actual CDF of an analytical distribution. The scale here is the standard deviation and the loc is the mean - sometimes scipy uses different names, and aren't that clear in the documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.ecdfplot(data=scipy.stats.expon.rvs(size=100, scale=np.std(df[\"Infections\"]), loc=np.mean(df[\"Infections\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_cdf = thinkstats2.Cdf(df[\"Infections\"])\n",
    "thinkplot.Cdf(inf_cdf)\n",
    "sns.ecdfplot(data=scipy.stats.expon.rvs(size=100, scale=np.std(df[\"Infections\"]), loc=np.mean(df[\"Infections\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we look at the PMF to see the \"normal\" distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a PMF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ....Ugh... Not Spectacular\n",
    "\n",
    "This is an illustration of one of the reasons that CDFs can be useful in comparing distributions. We can break it down:\n",
    "<ul>\n",
    "<li> If we look back up to the line plots, as we look towards the right side of the chart, in the climb, we obviously aren't taking on every value. One of exponential growth's things is jumping quickly. \n",
    "<li> Those values that are skipped, aren't in the distribution, so if we plot them on a histogram, or in this case - a PMF, there's nothing there, as they never occur. \n",
    "<li> A cummulative distribution doesn't have the same impact from this, the flat parts of the cummulative graph correspond to the zero parts of the PMF. In the CDF though, the pattern of the distribution just becomes somewhat more blocky, but maintains the same basic shape. \n",
    "    <ul>\n",
    "    <li> Binning also addresses this, but if we look at the actual value for number of infections, try to think of a bin size that makes sense for this situation. Test a few\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "So this CDF comparison is another tool in our toolkit - we don't need to use it, but when it does the job better, then we should. \n",
    "\n",
    "We'll also have different tools to deal with plotting distribution data soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Distributions\n",
    "\n",
    "The data looks like it shows an exponential distribution, more or less, and a steeply distributed one at that. Just like with the normal distribution that we can almost always eyeball against a histogram or pmf, this exponential distribution is pretty easy to pattern via the CDF. Each method is valid, it depends on the situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Part #1\n",
    "\n",
    "Use this data to:\n",
    "<ul>\n",
    "<li> Generate a CDF for the emperical data. \n",
    "<li> Generate a CDF for the equivalent analytical distribution.\n",
    "<li> Do they seem to match? \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = pd.read_csv(\"data/expon.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations - A Brief and Gentle Introduction\n",
    "\n",
    "One other thing we can do to match analytical distributions is to use <b>transformations</b> to make the data easier to deal with. Transformations are simply things that we can do to change, or transform, our data into a more usable format. This is common is some situations because certain things are just easier if the data is changed, particularly if it is changed into a known distribution later on. \n",
    "\n",
    "### Axis Scaling\n",
    "\n",
    "Looks pretty exponential! \n",
    "\n",
    "What if we were to make the y axis into a logarithmic scale. Why?\n",
    "<ul>\n",
    "The original formula is y = s * b^x:\n",
    "    <ul>\n",
    "    <li>y = # infections\n",
    "    <li>s = initial number of people infected\n",
    "    <li>b = infections generated per infected person. (Growth Rate)\n",
    "    <li>x = days in\n",
    "    </ul>\n",
    "</ul>\n",
    "Taking the log of both sides and looking at in on that scale looks a little different....\n",
    "y = log(s) + log(b) * x\n",
    "Note: the y stays y, and not log(y) when we graph it, because of that log scale.\n",
    "\n",
    "This is a transformation to the axis, which serves to \"eliminate\" the exponential curve, leaving us with a line. \n",
    "<b>This is now a linerar function! y=mx+b</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('COVID Gonna Give It To Ya')\n",
    "sns.lineplot(data=df, x=\"Day\", y=\"Infections\")\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for Exponential\n",
    "\n",
    "Now the line is kind of straight. The more straight it is, the \"better\". \n",
    "\n",
    "The closer this curve is to a line, the closer the real data is to exponential. Why..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate best fit line\n",
    "#You don't need to memorize this, we'll do this stuff later when we do scatter plots and regression. \n",
    "y = np.log(df[\"Infections\"])\n",
    "x = df[\"Day\"]\n",
    "m,b=np.polyfit(x, y, 1, w=np.sqrt(y))\n",
    "y_fit=np.exp(m*x+b)\n",
    "print(m)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show best fit line on graph.\n",
    "plt.title('COVID Gonna Give It To Ya')\n",
    "sns.lineplot(data=df, x=\"Day\", y=\"Infections\")\n",
    "sns.lineplot(x=x, y=y_fit, color=\"red\")\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? After the log transformation a straight line that we generated is a \"perfect\" exponential function - if we undid the log operation, we'd get that as a result. Our transformed data is a drunken stumble version of an exponential distribution (we know this, because we looked at the math above). The closer the emperical data is to the analytical version, the better a fit that analytical version is, just like the normal data vs a prbability plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Data and its Analytical Match\n",
    "\n",
    "What does all this mean? These general points are important! \n",
    "\n",
    "One of the uses of analytical models is to give us a simplied model that we can use to examine real data. \n",
    "The closer the data is to the model, the better it \"fits\". This allows us to utilize all that we know about that distribution to add to what we know from our data - if we know that our data follows a certain distribution (determined by matching our data to an analytical distribution) with certain parameters (measured from our emperical data), we can use this to do things like make predictions and generalizations based on a small(ish) emperical sample. We \"know\" the data will be distributed in a certain way, so all we need to do is adapt that to our specific inputs. \n",
    "\n",
    "Matching the distribution seen in our data to a known analytical distribution is a very important concept, especially as we move on to inferential (using stats to make predictions) statistics going forward. This is also why we can generalize things like polling from a small sample. We know, mathmatically, what to expect from an analytical distribution, so if we can \"fit\" our data to one of these distributions, we can use all that knowledge to draw conclusions on our data. \n",
    "\n",
    "Things like the log scale for exponential functions give us a way to look for fit. There's visual and analytical methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show best fit line on graph.\n",
    "plt.title('Distance between the real value and the prediction matters...')\n",
    "sns.scatterplot(data=df, x=\"Day\", y=\"Infections\")\n",
    "sns.lineplot(x=x, y=y_fit, color=\"red\")\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Part 2\n",
    "\n",
    "Plot the log-transformed version of the plot of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate best fit line\n",
    "#You don't need to memorize this, we'll do this stuff later when we do scatter plots and regression. \n",
    "ex_y = np.log(ex[\"Value\"])\n",
    "ex_x = ex[\"Index\"]\n",
    "ex_m, ex_b = np.polyfit(ex_x, ex_y, 1, w=np.sqrt(ex_y))\n",
    "ex_fit=np.exp(ex_m * ex_x + ex_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise Findings\n",
    "\n",
    "It looks like a good match early, then splits a bit at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Predictions\n",
    "\n",
    "We can \"undo\" the log part to see our new predictions! They aren't awesome, but we just started. \n",
    "\n",
    "The closer of a match that we can get of an analytical distribution to our emperical distribution, the more that analytical distribution will help us make generalizations. There are a few things we can do to improve this match:\n",
    "<ul>\n",
    "<li> Increase sample size - if possible, adding more sample data will allow us to better and more accurately match to an analytical distribution. \n",
    "<li> Use transformations - things like logs to make skewed normal distributions normal can help manipulate data to more closely match an analytical distribution. \n",
    "<li> Choose different distributions - we will look at a few, there are many. While the bulk of scenarios can be captured with a handful of distributions (normal, lognormal, exponential, pareto, etc...), there are a bunch that may fit your data. \n",
    "<li> Balance fit to the model - this is a concept that we'll revisit heavily when doing machine learning. In short, we want an analytical model that follows the same pattern as our emperical distribution - but is not <b>so</b> tailored to the data that it does things like follow singular outliers to the detriment of the overall fit. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Distance between the real value and the prediction matters...')\n",
    "sns.scatterplot(data=df, x=\"Day\", y=\"Infections\")\n",
    "sns.lineplot(x=x, y=y_fit, color=\"red\")\n",
    "plt.title('COVID Gonna Give It To Ya')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Exponential Dataset - You Can Ignore From Here Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = scipy.stats.expon.rvs(size=250, scale=5, loc=78)\n",
    "a.sort()\n",
    "sns.ecdfplot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(0,249,250)\n",
    "#b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.rand(1,250)*3\n",
    "#c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = a + c[0]\n",
    "d.sort()\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame([b,a]).T\n",
    "tmp_df.columns = [\"Index\", \"Value\"]\n",
    "tmp_df.sort_values(ascending=True, inplace=True, by=\"Value\")\n",
    "tmp_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.head(230).to_csv(\"data/expon.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
