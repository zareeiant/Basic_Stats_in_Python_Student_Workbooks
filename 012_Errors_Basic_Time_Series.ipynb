{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Stuff\n",
    "import thinkplot\n",
    "import thinkstats2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors\n",
    "\n",
    "Here we will look at error calculations. The book covers the same material in chapter 8 in a slightly different way. I think this is the most simple way to understand error, it also gives us a tiny peak into time series calculations - or situations where we have a dataset that is a series of values. \n",
    "\n",
    "Error is critically important as we move into inferential statistics and eventually into machine learning. We are generating predictions, so we need some way to objectively evaluate how accurate we can expect those predictions to be. Calculating error allows us to put a number on the accuracy of a model. If we remember back to some of the data that we used last time, we saw our first peek at errors when looking at regression lines. \n",
    "\n",
    "### Scatter Plot and Line of Best Fit\n",
    "\n",
    "We'll start with some familiar data from last time, and build a scatter plot with two of the correlated values. We can also generate a line of best fit, which is our model or prediction of what the value <i>should</i> be. This is technically the prediction from our machine learning model - a linear regression. We will fill more on creating a prediction in a few classes, for now the critical part is that we have two things plotted here:\n",
    "<ul>\n",
    "<li> The emperical data points. These are the \"true\" values, from our data. \n",
    "<li> The line of best fit. This is our model - the representation of the data. It is our \"prediction\" of what the Y value should be at any value of X. \n",
    "</ul>\n",
    "\n",
    "The examination of error looks at the differences between these - the less error, the closer the real data and our model are. \n",
    "\n",
    "<b>Note:</b> The linregress function can be pretty much ignored for now, we are just using it to get the slope and intercept. We'll dig into this more in a few weeks, for now it is just a shortcut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drg = pd.read_csv(\"data/drug-use-by-age.csv\")\n",
    "\n",
    "slope, intercept, r_value, pv, se = ss.linregress(drg[\"alcohol-use\"], drg[\"heroin-use\"])\n",
    "print(\"Slope:\", slope)\n",
    "print(\"Intercept:\", intercept)\n",
    "equation = \"Equation:  y=\", slope, \"* x +\", intercept\n",
    "#print(equation)\n",
    "\n",
    "sns.regplot(data=drg, x=\"alcohol-use\", y=\"heroin-use\", dropna=True, ci=0).set(title=equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highlight Errors\n",
    "\n",
    "Each error is the distance, in the y direction*, that the model is from each point. \n",
    "\n",
    "The regression line is a model - an analytical representation of real data. The model (the line) serves as our prediction - \"for an X value of _____, we expect a Y value of _______\", where that y value is the Y value of the model at X. Our model isn't perfect, for the values around 10-30, the model and the real data look to be very close, so a prediction from the model and the real value are virtually identical. For values around 40-50, there is a much larger separation - the model delivers predictions that are different from the emperical data. This vertical difference is the error, the closer the model is to the data, the less error there is, or the better it \"fits\". \n",
    "\n",
    "![Errors](images/error_size.png)\n",
    "\n",
    "*The \"in the y direction\" part is due to the X-Y variable choice and the convention of how we write functions. Here X is independent varaible, Y is the dependent, so the form of the line is y = m * X + b, a.k.a \"y is a function of X\". What this means is that we provide an X (or several X's in larger scenarios), and we predict a Y. So since we are predicting Y, we measure our error with respect to Y. \n",
    "\n",
    "<b>Note:</b> This is actually a really simple example of a machine learning model, and this error calculation is identical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - Time Series\n",
    "\n",
    "Our data is a list of gas prices - the average of several gas stations in one city. We have 30 days of data. If our goal is to predict the price on day 31 we have several ways we could do it - this is time series analysis that we'll look at a bit more later. For now, we'll look at the most simple way - take an average. \n",
    "\n",
    "My simple algorithm for predicting the next day's gas price will simply average together the previous 5 days of data, and that will be the prediction for the next day. This is kind of what we do in lots of situations - how much will a bake sale yeild? How long will it take to drive to Calgary? How many people will be ahead of me in line at Starbucks? We can average the previous few values together and use that as a projection. There obviously are more advanced ways to predict this, for now we'll use this simple way because we are just looking for error. \n",
    "\n",
    "#### What's a Time Series?\n",
    "\n",
    "This data is a time series, or data that progresses over time (day by day, year by year, hour by hour, etc...) Time series data is something that we'll touch on a little more later, and into ML. Dealing with time series data is somewhat different that other data when we get into predictive modelling. Here, all we need to care about is our model(s), the real data, and how much they differ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/GasTimeSeries.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average together last 5 values. \n",
    "avg5 = df[\"Price\"].tail(5).mean()\n",
    "avg5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a projected value for tomorrow. However, what if my friend says \"averaging 5 days is silly, I'm going to only use the past 3!\"\n",
    "\n",
    "This will give us two separate models - they each do the same thing, but they were each calculated differently, and will yeild different results. We can explore their error to try to determine which is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average together last 3 values. \n",
    "avg3 = df[\"Price\"].tail(3).mean()\n",
    "avg3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Predictions\n",
    "\n",
    "Ok, now we both have predictions for the price on day 31, but who's prediction is more accurate? We have no idea. \n",
    "\n",
    "The way that we can evaluate predictions against eachother is by running trials where we know the answer, and comparing our prediction to real values. This basic idea holds true for the machine learning stuff we do later on. The generallized process is:\n",
    "\n",
    "- Use some algorithms to create some models that make predictions.\n",
    "\n",
    "- Use those models on some old (test) data, where we know the actual, real outcome.\n",
    "\n",
    "- Compare our predictions to those known values - the closer we are, the more accurate that model is. \n",
    "\n",
    "- Choose the most accurate (usually - there's sometimes other factors) model, as its predictions are the best.\n",
    "\n",
    "\n",
    "\n",
    "So, for our gas price scenario here:\n",
    "\n",
    "- We have two models - a 3 day average and a 5 day average. We could have more, including sophisticated ones.\n",
    "\n",
    "- Run our old (test) data through the models to generate a prediction. We also know the correct values for these predictions.\n",
    "\n",
    "- For each prediction, calculate how wrong it is - the error. (Aka the residual).\n",
    "\n",
    "- Tally up the errors to a single metric of error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add old predictions\n",
    "#Note: to figure out the command I Googled \"dataframe moving average\" and got the rolling function\n",
    "df[\"avg5\"] = df[\"Price\"].rolling(5, closed=\"left\").mean()\n",
    "df[\"avg3\"] = df[\"Price\"].rolling(3, closed=\"left\").mean()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the line for day 6:\n",
    "\n",
    "- The real value is 1.48.\n",
    "\n",
    "- The 5 day model predicts 1.472.\n",
    "\n",
    "- The 3 day model predicts 1.457. \n",
    "\n",
    "So the 5 day model is off by .08, while the 3 day model is off by .23 - apx three times as much error. \n",
    "\n",
    "Every one of these old predictions will have an error like this, we can populate them in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the residuals (errors) and put them into the table\n",
    "df[\"res5\"] = df[\"Price\"]-df[\"avg5\"]\n",
    "df[\"res3\"] = df[\"Price\"]-df[\"avg3\"]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph them to take a look\n",
    "sns.regplot(x=\"Day\", y=\"Price\", data=df, label=\"Price\", fit_reg=False)\n",
    "sns.lineplot(x=\"Day\", y=\"avg5\", data=df, label=\"5 Day\")\n",
    "sns.lineplot(x=\"Day\", y=\"avg3\", data=df, label=\"3 Day\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphing Results\n",
    "\n",
    "On the plot we can see the emperical data, as well as our model. Since our models are both just averages of \"the last X days\", each seems to be a delayed shape of the emperical data. We can also see the error amounts visually on the graph very easily. The line is the real data, and the sets of dots are the models that we've created that model that data (and that could be used to look forward to make predictions.)\n",
    "\n",
    "The error is simply the gap between the real value and the model's value. For example, around day 26, the 3 Day Rolling Average model delivers a nearly perfect prediction - there is near 0 error. On day 11, both models deliver predictions that have lots of error, each one is approximately .10 too high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Day\"] == 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Day\"] == 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Residuals\n",
    "\n",
    "If we look at the two residual columns we can make some sense of it. If one model has smaller residuals, it is more accurate. \n",
    "\n",
    "It isn't really practical to use a massive column of errors, especially with a real amount of data, so we have ways to aggregate these individual errors into total metrics. For example, we can average all the errors together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"res5\", \"res3\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way is pretty dumb though, since if we predict 5c too high one time, and 5c too low another, that will average out to no error - obviously not really a good calculation in most cases. What if we take the absolute values, to remove that issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"res5\", \"res3\"]].abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does a better job, but...\n",
    "\n",
    "### Error Metrics and MSE\n",
    "\n",
    "Better, a more realistic estimate of the error. \n",
    "\n",
    "In most cases however, we don't use the absolute value to deal with negatives, we use squaring. There are a few reasons for this:\n",
    "\n",
    "- Simple: small errors are 'penalized' less than large errors when squaring. \n",
    "\n",
    "- Lazy: it is extremely common, well understood, and implemented all over the place. \n",
    "\n",
    "- Mathy: absolute value calculations generally suck when doing real math. Squaring gives the most 'efficient' estimation of errors - you need the smallest number of values to create an estimate, for math reasons we really don't care about: https://en.m.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem\n",
    "\n",
    "We can calculate a common error metric, MSE - mean squared error. Which is just what it sounds like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"res5\", \"res3\"]].pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very common error metric is RMSE - root mean squared error, which is just the square root of the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RMSE\n",
    "e5, e3 = df[[\"res5\", \"res3\"]].pow(2).mean()\n",
    "np.sqrt(e5), np.sqrt(e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE\n",
    "\n",
    "Similarly to how we usually translate variance to standard deviation, we usually translate MSE to RMSE. RMSE is in the \"same terms\" as the original data, we can think of it as rougly the expected amout of error in a typical prediction. \n",
    "\n",
    "For our purposes, going forward into machine learning, we are usually looking at error as our \"score\" - we want to create models that have less error, so their predictions are closer to reality, so we can rely on our predictions with more confidence. \n",
    "\n",
    "##### Results\n",
    "\n",
    "So, for these calculations the 3 day average model is more accurate with our data. With new data, there's no assurance that it will continue to be more accurate, but based on the data we have, we can trust it more. The more old (training) data we have, the more robust these error caclulations will generally be. \n",
    "\n",
    "We don't need to do this by hand, there are existing functions that do it for us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RMSE using prebuilt functions. Hopefully the results are the same!!\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# real value\n",
    "expected = df[\"Price\"]\n",
    "#Note: The iloc thing selects the rows. The function can't deal with missing values.\n",
    "# predicted value\n",
    "predicted5 = df[\"avg5\"].iloc[5:]\n",
    "predicted3 = df[\"avg3\"].iloc[3:]\n",
    "# calculate errors. Change squared to True to get MSE\n",
    "errors5 = mean_squared_error(expected.iloc[5:], predicted5, squared=False)\n",
    "errors3 = mean_squared_error(expected.iloc[3:], predicted3, squared=False)\n",
    "errors5, errors3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors - MSE and RMSE\n",
    "\n",
    "There are a lot of different ways to calculate the error, RMSE is probably the one that matters most for use, MSE is also used/seen sometimes. \n",
    "\n",
    "<b>Note:</b> MSE and RMSE can be really impacted by outliers. Think about why. Another common error measure - mean absolute error tends to be less impacted by outliers. Think about why. \n",
    "\n",
    "The basic idea of every error cacluation is just like this simple example - what is the prediction, what is the expected value, how much do they differ, how do we aggregate those differences into a value. (Classification errors are different, we'll worry about that later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thought Exercise\n",
    "\n",
    "We'll cover this later, so if it makes no sense please just skip it. \n",
    "\n",
    "This is a plot of the residuals for the first datasets above (X vs residual), called a residual plot. Does it appear that there is anything we can learn or conclude by looking at this plot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(data=drg, x=\"alcohol-use\", y=\"heroin-use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise for You!!\n",
    "\n",
    "We can try to calculate a new error metric - we want to replace the squaring in the RMSE error with a power to the 4. So instead of squaring the residuals, then later taking the square roots, we'll take the residuals to a power of 4, then later take the 4th root.\n",
    "\n",
    "We will call our new error metric - RM4E - root mean 4th'd error. Rolls off the tongue. \n",
    "\n",
    "### Challenge\n",
    "\n",
    "Use the line of best fit from the alcohol vs heroin chart above as the prediction. For this prediction, calculate:\n",
    "<ul>\n",
    "<li> The Mean 4th Error - Raise all residuals to the 4th power, and take their mean. Like the squaring in the MSE, except to the 4th rather than the 2nd power. \n",
    "<li> The Root Mean 4th Error - The 4th root of the M4E calculated above.\n",
    "<li> <b>Q:</b> What would happen if we did this to the power of 3 rather than 4?\n",
    "<li> <b>Q:</b> What are the impacts of doing this, vs the normal squaring?\n",
    "<li> <b>Q:</b> Are there any scenarios where this might be useful?\n",
    "</ul>\n",
    "\n",
    "Recall, the slope and intercept are already calculated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = drg[[\"alcohol-use\", \"heroin-use\"]]\n",
    "d2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Predictions from Model\n",
    "\n",
    "I will add a column that  is the prediction for each row. Recall, the prediction is the line of best fit that we generated above, or y = slope*x + intercept. We have the slope and intercept from above, so we can plug each x in to calculate the prediction from that formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Prediction to Check\n",
    "\n",
    "I will plot my data, along with the predictions, to make sure that things look realistic before going forward. This isn't required, it is just a check. If the plot showed data that didn't overlap, we'd know somehting has gone wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Residuals\n",
    "\n",
    "The predictions look as expected. Now I can calculate the residuals, or the error for each prediction versus the true y value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Residuals to the 4th\n",
    "\n",
    "We are calculating the mean of the errors to the 4th power, so I will add a column to raise the residuals to the 4th. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Error Totals\n",
    "\n",
    "We can average, and then 4th root the last column, to get our totals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with MSE/RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise # 2\n",
    "\n",
    "<ul>\n",
    "<li>Calculate a 4 day moving average and a 6 day moving average for the time series data above. \n",
    "<li>Which is a more reliable prediction model? Why?\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #3\n",
    "\n",
    "Too Easy? Try to build a rm4e function, that can return the error given a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. something along the lines of this... Details may differ. \n",
    "#def rm4e():\n",
    "#\n",
    "#   return rm4e_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
