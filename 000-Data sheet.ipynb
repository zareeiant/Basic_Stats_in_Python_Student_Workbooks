{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "#Helps check for null and none number values \n",
    "#Drop columns\n",
    "drg = drg.drop(columns=[\"n\", 'alcohol-frequency'])\n",
    "dfE.drop(columns={\"BodyFat\"}, inplace=True)\n",
    "#Rename column for teams\n",
    "hock = hock.rename(columns={'Unnamed: 1':\"Team\"})\n",
    "\n",
    "hw = hw.dropna(axis=0)\n",
    "\n",
    "#Are there nulls?\n",
    "df.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a scatterplot. \n",
    "thinkplot.Scatter(df[\"height\"], df[\"weight\"])\n",
    "thinkplot.Show(xlabel=\"Height\", ylabel=\"Weight\", axis=[120,220,20,160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add jitter. \n",
    "thinkplot.Scatter(thinkstats2.Jitter(df[\"height\"]), thinkstats2.Jitter(df[\"weight\"]), alpha=.3)\n",
    "thinkplot.Show(xlabel=\"Height\", ylabel=\"Weight\", axis=[120,220,20,160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate both correlation metrics\n",
    "a=df[\"height\"].values.tolist()\n",
    "b=df[\"weight\"].values.tolist()\n",
    "thinkstats2.Corr(a, b), thinkstats2.SpearmanCorr(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cut down data to only numeric values, and fliter out some outliers. \n",
    "df2 = df[[\"age\", \"gender\", \"height\", \"weight\", \"ap_lo\", \"ap_hi\"]]\n",
    "df2 = df2[(df2[\"height\"]>110) & (df2[\"height\"] < 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate pairplot\n",
    "sns.pairplot(df2,kind=\"reg\", dropna=True, hue=\"gender\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the data in our dataframe, and change it into a set of correlations. \n",
    "drg2 = drg.apply(pd.to_numeric, errors='coerce')\n",
    "drg2 = drg2.drop(columns=[\"age\"])\n",
    "drg2 = drg2.corr()\n",
    "drg2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Correlations in heatmap\n",
    "mask = np.triu(np.ones_like(drg2, dtype=bool))\n",
    "sns.heatmap(drg2, center=0, linewidths=.5, annot=True, cmap=\"YlGnBu\", yticklabels=True, mask=mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=d, x=\"Glucose\", kde=True, col=\"Outcome\", stat=\"density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Diabetic people are normal!\n",
    "d_neg = d[d[\"Outcome\"]==0]\n",
    "thinkstats2.NormalProbabilityPlot(d_neg[\"Glucose\"])\n",
    "\n",
    "\n",
    "\n",
    "#Create a normal distribution. Because we have stated \"it is normal\",\n",
    "norm = ss.norm(loc=d_neg[\"Glucose\"].mean(), scale=d_neg[\"Glucose\"].std())\n",
    "\n",
    "\n",
    "#Skewnorm is another distribution - a normal one with some skew. We calculate it, and pass it to our distribution with a=\n",
    "a = thinkstats2.PearsonMedianSkewness(d_neg[\"Glucose\"])\n",
    "norms = ss.skewnorm(loc=d_neg[\"Glucose\"].mean(), scale=d_neg[\"Glucose\"].std(), a=a)\n",
    "norms.cdf(6), norms.median(), norms.mean(), a\n",
    "\n",
    "#Create a model that is fitted to the data, automagically.\n",
    "ae, loce, scalee = ss.skewnorm.fit(d_neg[\"Glucose\"])\n",
    "snormFit = ss.skewnorm(ae, loce, scalee)\n",
    "\n",
    "#Plot all the distributions together. \n",
    "sns.distplot(norms.rvs(size=100000), color=\"red\")\n",
    "sns.distplot(norm.rvs(size=100000), color=\"blue\")\n",
    "sns.distplot(snormFit.rvs(size=100000), color=\"yellow\")\n",
    "sns.distplot(d_neg[\"Glucose\"], color=\"green\") #This is the actual data\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate regression plot \n",
    "slope, intercept, r_value, pv, se = ss.linregress(drg[\"alcohol-use\"], drg[\"heroin-use\"])\n",
    "print(\"Slope:\", slope)\n",
    "print(\"Intercept:\", intercept)\n",
    "equation = \"Equation:  y=\", slope, \"* x +\", intercept\n",
    "#print(equation)\n",
    "\n",
    "sns.regplot(data=drg, x=\"alcohol-use\", y=\"heroin-use\", dropna=True, ci=0).set(title=equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add old predictions\n",
    "#Note: to figure out the command I Googled \"dataframe moving average\" and got the rolling function\n",
    "df[\"avg5\"] = df[\"Price\"].rolling(5, closed=\"left\").mean()\n",
    "#Calculate the residuals (errors) and put them into the table\n",
    "df[\"res5\"] = df[\"Price\"]-df[\"avg5\"]\n",
    "\n",
    "\n",
    "\n",
    "#Calculate RMSE using prebuilt functions. Hopefully the results are the same!!\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# real value\n",
    "expected = df[\"Price\"]\n",
    "#Note: The iloc thing selects the rows. The function can't deal with missing values.\n",
    "# predicted value\n",
    "predicted5 = df[\"avg5\"].iloc[5:]\n",
    "predicted3 = df[\"avg3\"].iloc[3:]\n",
    "# calculate errors. Change squared to True to get MSE\n",
    "errors5 = mean_squared_error(expected.iloc[5:], predicted5, squared=False)\n",
    "errors3 = mean_squared_error(expected.iloc[3:], predicted3, squared=False)\n",
    "errors5, errors3\n",
    "\n",
    "# Add Residuals\n",
    "d2[\"residual\"] = d2[\"heroin-use\"]-d2[\"pred\"]\n",
    "#RMS  and RMSE\n",
    "d2[\"res2\"] = d2[\"residual\"]**2\n",
    "m2e = d2[\"res2\"].mean()\n",
    "rm2e = d2[\"res2\"].mean()**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanError(estimates, actual):\n",
    "    errors = [estimate-actual for estimate in estimates]\n",
    "    return np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(estimates, actual):\n",
    "    e2 = [(estimate-actual)**2 for estimate in estimates]\n",
    "    mse = np.mean(e2)\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate RMSE from residuals\n",
    "def rmseFromResiduals(residuals):\n",
    "    SSE = 0\n",
    "    for i in range(len(residuals)):\n",
    "        SSE += (residuals[i]**2)\n",
    "    MSE = SSE/len(residuals)\n",
    "    return math.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the RMSE, here using the statsmodel function. \n",
    "ypred = est2.predict(X2)\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "rmse(d[y1], ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sklearn RMSE\n",
    "mean_squared_error(d[y1], ypred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEstimates(mu=0, median=0, sigma=1, n=1000, iters=1000):\n",
    "    estimates1 = []\n",
    "    estimates2 = []\n",
    "    means = []\n",
    "    medians = []\n",
    "    for _ in range(iters):\n",
    "        xs = np.random.normal(mu, sigma, n)\n",
    "        xbar = np.mean(xs)\n",
    "        means.append(xbar)\n",
    "        xmed = np.median(xs)\n",
    "        medians.append(xmed)\n",
    "        biased = np.var(xs)\n",
    "        unbiased = np.var(xs, ddof=1)\n",
    "        estimates1.append(biased)\n",
    "        estimates2.append(unbiased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal Simulator\n",
    "def simulateNormal(mu=0, sigma=1, n=100, m=10000, ciLow=5, ciHi=95):\n",
    "    means = [] #List of all the means that are created. \n",
    "    preds = []\n",
    "    for j in range(m): #Run m number of simulations. \n",
    "        xs = np.random.normal(mu, sigma, n) #Generate a normal dist based on emperical data. \n",
    "        xbar = np.mean(xs) #Take the mean of the dist above's values\n",
    "        means.append(xbar) #Add to list. \n",
    "        preds.append(xs)\n",
    "    cdf = thinkstats2.Cdf(means) #Make a CDF of the means of the analytical dist's\n",
    "    ci = cdf.Percentile(ciLow), cdf.Percentile(ciHi) #5th, 95th percentiles. \n",
    "    muList = [mu] * m\n",
    "    stderr = mean_squared_error(means, muList, squared=False) #RMSE of how different the random analytical means are from the emp. mean. \n",
    "    return cdf, ci, stderr, means, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produce estimates - change the n and trials and observe results\n",
    "mu = df[\"log.annual.inc\"].mean()\n",
    "sig = df[\"log.annual.inc\"].std()\n",
    "n = df[\"log.annual.inc\"].count()\n",
    "med = df[\"log.annual.inc\"].median()\n",
    "print(\"Mean:\",mu,\" Median:\",med)\n",
    "\n",
    "cdf, ci, stderr, means, preds = simulateNormal(mu=med, sigma=sig, n=20, m=20, ciLow=5, ciHi=95)\n",
    "print(\"Standard Error:\", stderr)\n",
    "print(\"Low CI:\", ci[0])\n",
    "print(\"Hi CI:\", ci[1])\n",
    "print(\"Mean of Means:\", cdf.Mean())\n",
    "thinkplot.Cdf(cdf)\n",
    "thinkplot.axvline(cdf.Mean(), color=\"green\")\n",
    "thinkplot.axvline(ci[0], color=\"red\")\n",
    "thinkplot.axvline(ci[1], color=\"red\")\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponential simulator\n",
    "def simulateExp(mean=1, n=100, m=10000, ciLow=5, ciHi=95):\n",
    "    means = []\n",
    "    for j in range(m):\n",
    "        xs = np.random.exponential(1/mean,n)\n",
    "        xbar = 1/np.mean(xs)\n",
    "        means.append(xbar)\n",
    "    cdf = thinkstats2.Cdf(means)\n",
    "    ci = cdf.Percentile(ciLow), cdf.Percentile(ciHi)\n",
    "    rateList = [mean] * m\n",
    "    stderr = mean_squared_error(means, rateList, squared=False)\n",
    "    return cdf, ci, stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot em\n",
    "sns.histplot(dPos, kde=True, stat=\"density\")\n",
    "sns.histplot(dNeg, kde=True, stat=\"density\", color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulate one Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimGame(lam):\n",
    "    dist = ss.poisson(lam)\n",
    "    score = dist.rvs(1)\n",
    "    return score[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulate Many Games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateGame(lam=2., m=1000):\n",
    "    estimates = []\n",
    "    for i in range(m):\n",
    "        L = SimGame(lam)\n",
    "        estimates.append(L)\n",
    "\n",
    "    print('Goal Distribution:')\n",
    "    print('rmse Goals:', RMSE(estimates, lam))\n",
    "    print('mean error Goals:', MeanError(estimates, lam))\n",
    "    \n",
    "    pmf = thinkstats2.Pmf(estimates)\n",
    "    thinkplot.Hist(pmf)\n",
    "    thinkplot.Config(xlabel='Goals scored', ylabel='PMF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate a bunch of games, count how many one team wins vs the other. \n",
    "def estimateMatch(team1=1, team2=1, m=1000):\n",
    "    team1Wins = []\n",
    "    ties = []\n",
    "    for i in range(m):\n",
    "        goal1 = SimGame(team1)\n",
    "        goal2 = SimGame(team2)\n",
    "        team1Wins.append(int(goal1>goal2))\n",
    "        ties.append(int(goal1 == goal2))\n",
    "    return team1Wins, ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "oil_win, oil_ties = estimateMatch(gfOil,gfFla,1000)\n",
    "np.mean(oil_win), np.mean(oil_ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How likely is it for someone to vote Trump and Biden?\n",
    "probT = .46/(.49+.46)\n",
    "print(\"Trump Prob:\", probT)\n",
    "\n",
    "probB = .49/(.49+.46)\n",
    "print(\"Biden Prob:\", probB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVote(probCalc):\n",
    "    vote = np.random.binomial(n=1, p=probCalc)\n",
    "    return vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSample(voteProb, n=1000):\n",
    "    vote_list = []\n",
    "    for i in range(n):\n",
    "        vote_list.append(oneVote(voteProb))\n",
    "    return vote_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = getSample(probB, 5000)\n",
    "np.mean(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do Some Simulating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamples(voteProb, n=1000, samples=100, ciLow=2.5, ciHi=97.5):\n",
    "    meanList = []\n",
    "    for i in range(samples):\n",
    "        meanList.append(np.mean(getSample(voteProb, n)))\n",
    "    muList = [voteProb] * samples\n",
    "    cdf = thinkstats2.Cdf(meanList) #Make a CDF of the means of the analytical dist's\n",
    "    ci = cdf.Percentile(ciLow), cdf.Percentile(ciHi) #5th, 95th percentiles. \n",
    "    stderr = mean_squared_error(meanList, muList, squared=False)\n",
    "    return meanList, stderr, cdf, ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Winner!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a bunch of trials and count how many Biden wins in. \n",
    "trials = 1000\n",
    "means, err, cdfFin, ciFin = getSamples(probB, n=700, samples=trials)\n",
    "lowest = means - err\n",
    "highest = means + err\n",
    "bWins = 0 \n",
    "for i in range(len(means)):\n",
    "    if means[i] > .5000:\n",
    "        bWins = bWins + 1\n",
    "print(bWins/trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.Cdf(cdfFin)\n",
    "thinkplot.axvline(ciFin[0], color=\"red\")\n",
    "thinkplot.axvline(ciFin[1], color=\"red\")\n",
    "print(\"Fraction of Scenarios where Biden wins is %.1f%% \" % ((1-cdfFin.Prob(.50))*100))\n",
    "print(\"We are 95 percent confident that Biden will get between %.1f%% and %.1f%% percent of the vote\" % (ciFin[0]*100, ciFin[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, pval = ss.ttest_ind(dPos, dNeg, alternative=\"greater\")\n",
    "pval\n",
    "\n",
    "# Force Permutations\n",
    "stat2, pval2 = ss.ttest_ind(dPosBP, dNegBP, permutations=100)\n",
    "stat2, pval2\n",
    "\n",
    "# mw\n",
    "print(ss.mannwhitneyu(fv, boost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Check for signifigance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = .05\n",
    "if pval < cutoff:\n",
    "    print(\"Reject null hypothesis - Effect appears significant\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis - Effect may be due to random chance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test by hand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dPG = dfP.BloodPressure\n",
    "dNPG = dfNP.BloodPressure\n",
    "\n",
    "gP, gNP = dPG.mean(), dNPG.mean()\n",
    "print(gP, gNP)\n",
    "gDiff = abs(gP - gNP)\n",
    "print(gDiff, '<---That is my test stat. There is a diff in BP of this much, does it matter or is this difference likely to be random chance?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines the two groups.\n",
    "#Get the length of one for splitting below. \n",
    "allG = np.hstack((dPG, dNPG))\n",
    "nP = len(dNPG)\n",
    "\n",
    "#Generate random splits, and add the differnce in their means to a list. \n",
    "diffsP = []\n",
    "#Write the loop. For each of 100 trials, shuffle the list of BP, then split it into two. \n",
    "for i in range(100):\n",
    "    np.random.shuffle(allG)\n",
    "    data1 = allG[nP:]\n",
    "    data2 = allG[:nP]\n",
    "    tmpDiff = abs(data1.mean()-data2.mean())\n",
    "    diffsP.append(tmpDiff)\n",
    "diffsP[0:5], \"<--- Some Example Differences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the differences in means from the random samples,\n",
    "#Count how many times that difference exceeds our test stat. \n",
    "successP = 0\n",
    "#Write the loop. Just like the previous one. \n",
    "for i in range(len(diffsP)):\n",
    "    if diffsP[i] > gDiff:\n",
    "        successP += 1\n",
    "pP = successP/len(diffsP)\n",
    "print(\"P =\",pP)\n",
    "#Check muh Null Hyp!!\n",
    "cutoffP = .05\n",
    "if pP < cutoffP:\n",
    "    print(\"Reject null hypothesis - Effect appears significant\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis - Effect may be due to random chance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CohenEffectSize(group1, group2):\n",
    "    \"\"\"Compute Cohen's d.\n",
    "\n",
    "    group1: Series or NumPy array\n",
    "    group2: Series or NumPy array\n",
    "\n",
    "    returns: float\n",
    "    \"\"\"\n",
    "    diff = group1.mean() - group2.mean()\n",
    "\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statsmodels calculation of power. This also indicates that we are making a type 2 error as well \n",
    "from statsmodels.stats.power import TTestPower\n",
    "powerTest = TTestPower()\n",
    "ces = thinkstats2.CohenEffectSize(fAge, mAge)\n",
    "alpha = .05\n",
    "nobs = mCount + fCount #number of observations \n",
    "\n",
    "pow = powerTest.power(effect_size=ces, nobs=nobs, alpha=alpha)\n",
    "pow, ces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate n\n",
    "n = powerTest.solve_power(ces, power=.8, nobs=None, alpha=alpha)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What About Categorical Data - Chi-2 Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [10,10,10,10,10,10]\n",
    "observed = [8,9,19,5,8,11]\n",
    "ss.chisquare(observed, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if There's 3 or More Samples? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df2, x=\"Educ\", y=\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varStat, varP = ss.levene(df2['Age'][df2['Educ'] == 1],\n",
    "               df2['Age'][df2['Educ'] == 2],\n",
    "               df2['Age'][df2['Educ'] == 3],\n",
    "               df2['Age'][df2['Educ'] == 4],\n",
    "               df2['Age'][df2['Educ'] == 5])\n",
    "varP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.var(df2['Age'][df2['SES'] == 1.0]))\n",
    "print(np.var(df2['Age'][df2['SES'] == 2.0]))\n",
    "print(np.var(df2['Age'][df2['SES'] == 3.0]))\n",
    "print(np.var(df2['Age'][df2['SES'] == 4.0]))\n",
    "print(np.var(df2['Age'][df2['SES'] == 5.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.f_oneway(df2['Age'][df2['Educ'] == 1],\n",
    "               df2['Age'][df2['Educ'] == 2],\n",
    "               df2['Age'][df2['Educ'] == 3],\n",
    "               df2['Age'][df2['Educ'] == 4],\n",
    "               df2['Age'][df2['Educ'] == 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if the Data Isn't Normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df2, x=\"CDR\", hue=\"M/F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr_m = df2['CDR'][df2['M/F'] == \"M\"]\n",
    "cdr_f = df2['CDR'][df2['M/F'] == \"F\"]\n",
    "\n",
    "ss.mannwhitneyu(cdr_m, cdr_f)#This is a non parametric T=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, x=\"Day\", y=\"unvaccinated\", ci=0, color='blue')\n",
    "sns.lineplot(data=df, x=\"Day\", y=\"fully_vaccinated\", ci=0,color='red')\n",
    "sns.lineplot(data=df, x=\"Day\", y=\"one_booster\", ci=0, color='green')\n",
    "sns.lineplot(data=df, x=\"Day\", y=\"two_boosters\", ci=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear Least Squares</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yD = np.array(dfYd).reshape(-1,1)\n",
    "xD = np.array(dfXd).reshape(-1,1)\n",
    "xD.shape, yD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do regression\n",
    "inter, slope = thinkstats2.LeastSquares(hw[h], hw[w]) # Calculate model it\n",
    "res = thinkstats2.Residuals(hw[h], hw[w], inter, slope) # Make residual list. Used later, not needed now. \n",
    "regLine = thinkstats2.FitLine(hw[h], inter, slope) # Generate line for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph it.\n",
    "#The fitline above calculates the line for us. For an exercise, try to implement a copy of that...\n",
    "sns.scatterplot(x=hw[h], y=hw[w])\n",
    "sns.lineplot(x=regLine[0], y=regLine[1], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot residuals directly\n",
    "sns.scatterplot(x=hw[h], y=res)\n",
    "plt.axhline(0, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seaborn also has a built in regression plot. \n",
    "sns.regplot(x=hw[h], y=hw[w], ci=0)\n",
    "\n",
    "#Seaborn also has a built in residual plot. \n",
    "sns.residplot(x=hw[h], y=hw[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resNFL = thinkstats2.Residuals(hw2[h2], hw2[w2], int2, slope2)\n",
    "sns.scatterplot(y=resNFL, x=hw2[h2])\n",
    "plt.axhline(0, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineNFL = thinkstats2.FitLine(hw2[h2], int2, slope2)\n",
    "sns.scatterplot(x=hw2[h2], y=hw2[w2])\n",
    "sns.lineplot(x=lineNFL[0], y=lineNFL[1], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmseFromResiduals(residuals):\n",
    "    SSE = 0\n",
    "    for i in range(len(residuals)):\n",
    "        SSE += (residuals[i]**2)\n",
    "    MSE = SSE/len(residuals)\n",
    "    return math.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Packages for Regression    SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deal with data\n",
    "#ensure that the inputs and outputs are the right shape.\n",
    "#The -1 means basically \"make it one column\" in this use. \n",
    "x = np.array(hw[h]).reshape(-1,1)\n",
    "y = np.array(hw[w]).reshape(-1,1)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Model\n",
    "# Create an instance of a linear regression model and fit it to the data with the fit() function:\n",
    "#intercept2, slope2 = thinkstats2.LeastSquares(breath[\"oxigen_per_lit\"], breath[\"peaks_diff\"])\n",
    "model = LinearRegression().fit(x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the results of generating the model\n",
    "\n",
    "#Note: some results come wrapped in an array, that's what the [0]s are for. Remove them to see the true return. \n",
    "# Obtain the coefficient of determination by calling the model with the score() function, then print the coefficient of determination to see how good our model is:\n",
    "r_sq = model.score(x, y)\n",
    "print('Coefficient of determination (more on this later):', r_sq)\n",
    "# Print the Intercept:\n",
    "print('Intercept:', model.intercept_[0])\n",
    "# Print the Slope:\n",
    "print('Slope:', model.coef_[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions. \n",
    "#Note - you need to provide the values to predict in an array that is one column wide. \n",
    "#Generally you'd make an array of all the things you want to predict and do them en-masse. The [0] is gettign ride of the square brackets. they are used for indexing \n",
    "#It grabs the first item of the array \n",
    "print(\"A 183cm tall person is expected to be:\", model.predict(np.array(183).reshape(-1,1))[0][0], \"kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heightsToPredict = np.array([73, 65, 76, 98, 99])\n",
    "heightsToPredict = heightsToPredict.reshape(5,1)\n",
    "heightsToPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNFL.predict(heightsToPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph it\n",
    "# I'll make a df for ease of use. \n",
    "dat = pd.DataFrame(data=np.column_stack((x,y)),columns=['X','Y'])\n",
    "sns.scatterplot(data=dat, x=\"X\", y=\"Y\")\n",
    "\n",
    "#Generate the line\n",
    "inter = model.intercept_[0]\n",
    "slo = model.coef_[0][0]\n",
    "inter, slo\n",
    "lineInf = thinkstats2.FitLine(dat[\"X\"], inter, slo)\n",
    "sns.lineplot(x=lineInf[0], y=lineInf[1], color=\"red\")\n",
    "\n",
    "#Probably easier to just use a regplot! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X2 = sm.add_constant(x)\n",
    "est = sm.OLS(y, X2) #ordinary least square \n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPred = np.array([183, 208, 175])\n",
    "x_test = sm.add_constant(toPred)\n",
    "ypred = est2.predict(x_test)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2 - Coefficient of Determination\n",
    "\n",
    "These two values - RMSE and R2 - are the two primary (mostly - more on this later, along with R2 limitations) metrics that we can use to evaluate a model's accuracy and its predictive value. If we have a high R2 and a (comparatively) low RMSE, we are probably making reliable predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rSquared(yvals, residuals):\n",
    "    return (1-(thinkstats2.Var(residuals)/thinkstats2.Var(yvals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rSquared_2(yvals, residuals):\n",
    "    RSS = 0\n",
    "    TSS = 0\n",
    "    ybar = yvals.mean()\n",
    "    for i in range(len(residuals)):\n",
    "        RSS += (residuals[i]**2)\n",
    "        TSS += ((yvals[i]-ybar)**2)\n",
    "    r2 = 1-(RSS/TSS)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmseFromResiduals(residuals):\n",
    "    SSE = 0\n",
    "    for i in range(len(residuals)):\n",
    "        SSE += (residuals[i]**2)\n",
    "    MSE = SSE/len(residuals)\n",
    "    return math.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ex = ex[[\"Wrist\", \"BodyFat\"]]\n",
    "sns.pairplot(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression data setup\n",
    "dfYd = ex[\"BodyFat\"]\n",
    "dfXd = ex[\"Wrist\"]\n",
    "\n",
    "yD = np.array(dfYd).reshape(-1,1)\n",
    "xD = np.array(dfXd).reshape(-1,1)\n",
    "xD.shape, yD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "xTrainD,xTestD,yTrainD,yTestD = train_test_split(xD,yD,test_size=.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "modelD = LinearRegression().fit(xTrainD, yTrainD) \n",
    "predsD = modelD.predict(xTestD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE/R2\n",
    "print(\"RMSE\", mean_squared_error(yTestD, predsD, squared=False))\n",
    "print(\"R2\", modelD.score(xTestD, yTestD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y data is simple, do that first\n",
    "y = np.array(df[\"BodyFat\"]).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.drop(columns={\"BodyFat\"})\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df_)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate model \n",
    "model = LinearRegression().fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get some info on our new regression model\n",
    "r_sq = model.score(xTest, yTest)\n",
    "print('R-squared:', r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get RMSE\n",
    "tmp = model.predict(xTest)\n",
    "mean_squared_error(tmp, yTest, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Residuals and picture them in a DF for easy reading. \n",
    "tmp1 = pd.DataFrame(yTest, columns={\"Y values\"})\n",
    "tmp2 = pd.DataFrame(tmp, columns={\"Predictions\"})\n",
    "tmp3 = pd.DataFrame((yTest-tmp), columns={\"Residual\"})\n",
    "resFrame = pd.concat([tmp1,tmp2,tmp3], axis=1)\n",
    "resFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examin thresh\n",
    "\n",
    "posMean, negMean, posSTD, negSTD, posMed, negMed = dPos.mean(), dNeg.mean(), dPos.std(), dNeg.std(), dPos.median(), dNeg.median()\n",
    "simple_thresh = (posMean + negMean) / 2\n",
    "simple_thresh\n",
    "\n",
    "#or\n",
    "\n",
    "thresh = (posSTD * negMean + negSTD * posMean) / (posSTD + negSTD)\n",
    "thresh\n",
    "\n",
    "\n",
    "pos_below_thresh = sum(dPos < thresh)\n",
    "pos_below_thresh\n",
    "\n",
    "neg_above_thresh = sum(dNeg > thresh)\n",
    "neg_above_thresh\n",
    "\n",
    "pos_overlap = pos_below_thresh / len(dPos)\n",
    "neg_overlap = neg_above_thresh / len(dNeg)\n",
    "pos_overlap, neg_overlap\n",
    "\n",
    "misclassification_rate = (pos_overlap + neg_overlap) / 2\n",
    "misclassification_rate\n",
    "\n",
    "#Superiority\n",
    "\n",
    "sum(x > y for x, y in zip(dPos, dNeg)) / len(dPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CohenEffectSize(group1, group2):\n",
    "    \"\"\"Compute Cohen's d.\n",
    "\n",
    "    group1: Series or NumPy array\n",
    "    group2: Series or NumPy array\n",
    "\n",
    "    returns: float\n",
    "    \"\"\"\n",
    "    diff = group1.mean() - group2.mean()\n",
    "\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting cohen effect size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_superiority(control, treatment, n=1000):\n",
    "    \"\"\"Estimates overlap and superiority based on a sample.\n",
    "    \n",
    "    control: scipy.stats rv object\n",
    "    treatment: scipy.stats rv object\n",
    "    n: sample size\n",
    "    \"\"\"\n",
    "    control_sample = control.rvs(n)\n",
    "    treatment_sample = treatment.rvs(n)\n",
    "    thresh = (control.mean() + treatment.mean()) / 2\n",
    "    \n",
    "    control_above = sum(control_sample > thresh)\n",
    "    treatment_below = sum(treatment_sample < thresh)\n",
    "    overlap = (control_above + treatment_below) / n\n",
    "    \n",
    "    superiority = (treatment_sample > control_sample).mean()\n",
    "    return overlap, superiority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pdf(rv, num=4):\n",
    "    mean, std = rv.mean(), rv.std()\n",
    "    xs = np.linspace(mean - num*std, mean + num*std, 100)\n",
    "    ys = rv.pdf(xs)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdfs(cohen_d=2):\n",
    "    \"\"\"Plot PDFs for distributions that differ by some number of stds.\n",
    "    \n",
    "    cohen_d: number of standard deviations between the means\n",
    "    \"\"\"\n",
    "    control = ss.norm(0, 1)\n",
    "    treatment = ss.norm(cohen_d, 1)\n",
    "    xs, ys = eval_pdf(control)\n",
    "    plt.fill_between(xs, ys, label='control', color='C1', alpha=0.5)\n",
    "\n",
    "    xs, ys = eval_pdf(treatment)\n",
    "    plt.fill_between(xs, ys, label='treatment', color='C0', alpha=0.5)\n",
    "    \n",
    "    o, s = overlap_superiority(control, treatment)\n",
    "    plt.text(0, 0.05, 'overlap ' + str(o))\n",
    "    plt.text(0, 0.15, 'superiority ' + str(s))\n",
    "    plt.show()\n",
    "    #print('overlap', o)\n",
    "    #print('superiority', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCohen = CohenEffectSize(dPos, dNeg)\n",
    "plot_pdfs(dCohen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Odds, we can use ratios to make sense of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probability of being overweight\n",
    "pPos = sum(dPos >= 30)/len(dPos)\n",
    "pNeg = sum(dNeg >= 30)/len(dNeg)\n",
    "pPos, pNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate odds\n",
    "oddPos = pPos/(1-pPos)\n",
    "oddNeg = pNeg/(1-pNeg)\n",
    "oddPos, oddNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de7dd8cf6857084a9743b3a9718e5a99a869ef48e4b7a1d13fa5f97102dba3da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
