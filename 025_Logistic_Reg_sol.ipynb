{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load some data. We'll use this in a bit. \n",
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Linear regression allows us to make numerical predictions based on one or more inputs, either numerical or categorical. Logistic regression is the equivalent that allows us to make classfication predictions - predicting if something falls into group A or group B. \n",
    "\n",
    "Logistic regression is based on our old friend, the logit, of log ratio fame. We effectively are doing a regression to predict the likelihood of something happening, then categorizing it based on if it is more probable than some cutoff (e.g. 50%). For example, we can calculate the probability that a transaction is fraudulent, then if it is more likely than not to be fraud, we categorize it as such. For now, we'll look at predicting between two classes, but that's not a limit, we can categorize into many classes. \n",
    "\n",
    "Logistic regression (and other classification methods) are extremely common. Regression/prediction and classification are the two big pillars of predictive analytics that we will look at through next term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classification Problem\n",
    "\n",
    "Dealing with classification is a little different than regression, because now we are not looking to predict a value, we are looking to predic a class - or phrased alternatively, we are looking to divide two (or more) sets of data.  \n",
    "\n",
    "If we plot a simple 2 varaible problem, just like we did in linear regression, we'll get something that looks like this:\n",
    "<ul>\n",
    "<li> Suppose that BMI is our X and Outcome (do you have diabetes?) is the Y. \n",
    "<li> Plot that on a scatter plot. \n",
    "<li> Our goal is to use X to predict Y, just as it was in linear regression. \n",
    "<li> However, there's not a very obvious way to use the X value only to do a linear regression that has any degree of accuracy.\n",
    "    <ul>\n",
    "    <li> Seriously, try to generate any line of best fit that doesn't have massive residuals. \n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='BMI', ylabel='Outcome'>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=df, x=\"BMI\", y=\"Outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So What Do We Do?\n",
    "\n",
    "We need something that can transform our simple linear fitting into something... else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember the Logit?\n",
    "\n",
    "Recall from the probability stuff, we also calculated odds - or the ratio of something happening to it not happening. Odds are just an alternative expression of probability. E.g. if we are 30% likely to win a price, p = .3. We generally treat classification as splitting our data into 1 and 0, or true and not true, so the odds of the \"other option\" are allways just 1-odds. We can classfy into multiple categories (e.g. predict hair color as blonde, red, black, brown), the same ideas apply - we will examine this more next semester, for now it is all yes/no. If we translate that to odds:\n",
    "\n",
    "$ Odds = \\frac{.3}{(1-.3)} = \\frac{p}{(1-p)} = o $\n",
    "\n",
    "If we then take the log of that (remember, taking the log of the odds made things work), we end up with:\n",
    "\n",
    "$ log(odds) = log(\\frac{p}{(1-p)}) = log(\\frac{p}{not p}) = log(\\frac{prob(1)}{prob(0)}) $ (we can have two outcomes here - 1 or 0, so the probabilities are for either of those two happening)\n",
    "\n",
    "In logistic regression, we use the standard form:\n",
    "\n",
    "$ log(odds) = m*x + b $\n",
    "\n",
    "So, we are able to predict the log odds using the same linear regression format that we are used to. Thanks log odds! \n",
    "\n",
    "#### Full Math\n",
    "\n",
    "![Logistic 1](images/logit_math_1.jpeg \"Logistic 1\")\n",
    "![Logistic 2](images/logit_math_2.jpeg \"Logistic 2\")\n",
    "\n",
    "## Enter the Sigmoid\n",
    "\n",
    "We end up here with a function called the sigmoid, which is what gives us our actual predictions. The sigmoid has the useful property that it \"jumps\" from 0 to 1 very quickly and never goes past those bounds. \n",
    "\n",
    "![Sigmoid](images/sigmoid.png \"Sigmoid\")\n",
    "\n",
    "What this all means is that we can calculate the probability - which is the output of the sigmoid - then just draw a cutoff to label our prediction as 1 or 0. \n",
    "\n",
    "![Classification](images/sig_prob.png \"Classification\")\n",
    "\n",
    "Since the sigmoid is so \"steep\", it tends to do a good job of separating - small changes in the input, while the values are near the transition part of the curve, yeild large changes in the prediction. \n",
    "\n",
    "<b>Note:</b> we don't need to derive this or manually calculate it, we mainly just want to understand the concept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough Example\n",
    "\n",
    "Simple example... We want to predict Y, given some values of X. For this, we can say that the values are as follows:\n",
    "\n",
    "<ul>\n",
    "<li> Y = Passed high school. 1 = Yes.\n",
    "<li> X1 = Attended class. 1 = Yes.\n",
    "<li> X2 = Studied at home. 1 = Yes. \n",
    "</ul>\n",
    "\n",
    "Each set of values (a column) is one person, so we have two people who passed and two who did not. The details of the data don't matter much, we're looking at the mechanics here. We'll do a real one in a min. This middle part of the curve is sometimes called the Decision Boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "y = np.array([0, 1, 0, 1])\n",
    "x1 = np.array([0, 0, 0, 1])\n",
    "x2 = np.array([0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a regression, using the logit formula:\n",
    "\n",
    "$\\log o = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 $\n",
    "\n",
    "We don't know our coefficients though - the process for determining them isn't a direct calculation like linear regression. Here we need to try some, check our error, then improve. (This is a common thing in ML).\n",
    "\n",
    "For this, we are making an arbitrary guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [-1.5, 2.8, 1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate it out, just like a linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5, -0.4, -0.4,  2.4])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_o = b[0] + b[1] * x1 + b[2] * x2\n",
    "log_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert log odds to odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22313016,  0.67032005,  0.67032005, 11.02317638])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = np.exp(log_o)\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert odds to probabilities. These probabilities are the outputs of the sigmoid calculation, and we can use them to classify by just labeling things that are over the cutoff (usually .5) as 1s and the things that are under as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18242552, 0.40131234, 0.40131234, 0.9168273 ])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = o / (o+1)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Real Usage\n",
    "\n",
    "Those are all the predicted probabilities of each person passing high school. \n",
    "\n",
    "Now, a little weirdness. We started out this whole thing with some pretty random values for all the coefficients, so why would we trust these predictions? Well, right now, we wouldn't. What we need to do to make a model that is actually accurate is to check how well we did now, then make some improvements. \n",
    "\n",
    "To check how well we did now, we can calculate how close the probabilites are to the real values. E.g. Person #4 did really graduate, and our model predicted a ~92% chance of them graduating, that's good. Person #2 also graduated, but our model only predicted a ~40% chance of them graduating, that's bad. So our metric for evaluating is that we want our predictions to be as close as possible to the real values - or we want \"1\"s to have high percentages, and \"0\"s to have low percentages. The more sharpely we can discriminate between passes and fails, the more accurate the model. \n",
    "\n",
    "We can calculate this overall accuracy pretty simply - how likely are we to predict the correct answer? \n",
    "\n",
    "### Cost and Loss\n",
    "\n",
    "Some new concpets that are introduced here, and are important going forward, are the ideas of cost and loss. When doing these types of iteritive training processes our progress is tracked by our loss, or the amount of error. This amount is calculated by our loss function, or how that error is calculated. This is directly comparable to the MSE/RMSE process we looked at previously, we have some calculation to determine our overall accuracy. \n",
    "\n",
    "Each time we do an iteration, we get some amount of error, or loss. The best solution is where this loss is at it's lowest. This is the same idea as how the best linear regression model is best when the loss - the linear least squares distance - is at it's lowest. \n",
    "\n",
    "<b>Note:</b> the terms cost and loss are often used interchangably, and this is generally fine. Technically the loss function is for each specific example, and the cost is the overall summary. In practice, it is not that big of a deal to swap the terms - there won't really be many, if any, scenarios in which that will be confusing. \n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is the way that many algorithms try to minimize their loss. In short, this process is just:\n",
    "<ul>\n",
    "<li> Create a model. \n",
    "<li> Measure the loss. \n",
    "<li> Adjust the model's values (i.e. the slopes and intercepts here)\n",
    "<li> Check the loss again. \n",
    "<li> Repeat until you reach the lowest value for loss. (i.e. the most accurate model)\n",
    "</ul>\n",
    "\n",
    "![Gradient](images/grad_desc.png \"Gradient\")\n",
    "\n",
    "The algorithm \"knows which way to go\" in adjusting the weights between each trial via some calculus and matrix math that we will peek into when we look at neural networks. The algorithm can basically use partial derivitives to attribute error to the different values (the slopes), as well as if they are too high or too low. Each step moves these values a little, then we recheck. \n",
    "\n",
    "We will explore the details of gradient descent much more as we get into the machine learning stuff, for now understanding the general idea is good enough. This process is how most machine learning models \"learn\", and this is what is going on when they are processing for a long time. Each iteration moves the results (hopefully) to a point where the model has a little less error, and eventually we either \"find the bottom\" - which in logistic regression is the slopes and intercept values for the log-odds regression above, or we hit a limit of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is our Loss?\n",
    "\n",
    "Our loss in the example above is a calculation the summarizes all of our individual errors. In the previous cell we calculated the probability of each person passing (being 1), our original Y data shows us the true probability of each person passing (either 0 or 1). Each prediction has an error of the distance between that true value and our expected probability. \n",
    "\n",
    "E.g. for the second item, this person passed, so the real value is 1. We predicted a ~40% likelihood of them passing, so our error there is ~60%. Person 4 passed, we predicted a ~92% chance of them passing, so our error is ~8%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18242552, 0.59868766, 0.40131234, 0.0831727 ])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#likes = np.where(y, p, 1-p)\n",
    "likes = np.where(y, 1-p, p)\n",
    "likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function\n",
    "\n",
    "These individual accuracies can be tallied up, we'll do a simple one with a common loss function called log-loss. There are a bunch of \"real\" loss functions that we can use, we'll explore them later on in machine learning. The most simple one is also based on the log of the odds, it is called Binary Cross-entropy, or Log-Loss. Don't worry about these details too much now, we will explore this later. \n",
    "\n",
    "![Log Loss](images/log_loss.png \"Log Loss\")\n",
    "\n",
    "The goal of the algorithm is to find the smallest value for this totalled loss, that's when we are most accurate overall. \n",
    "\n",
    "We can turn this loss total into an overall cost by just dividing by n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42856998373415184"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_sum = 0\n",
    "for i in range(len(p)):\n",
    "    if y[i] == 1:\n",
    "        loss_sum += -np.log(p[i])\n",
    "    elif y[i] == 0:\n",
    "        loss_sum += -np.log(1-p[i])\n",
    "cost = loss_sum/len(p)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall goal is to find the lowest possibility for this value. The lower this value, the closer to reality our model was predicting, the more accurate we can be. The process is to now to:\n",
    "<ul>\n",
    "<li> Take this amount of overall error, use it to make an adjustment to those starting values. (The ones we made up to start)\n",
    "    <ul>\n",
    "    <li> This step is something we'll look at in more depth with neural networks. In involves some partial derivitives which (kind of) allow us to work backwards and attribue parts of the errors to the original inputs. \n",
    "    </ul>\n",
    "<li> Calculate the new error with the different starting point. \n",
    "<li> Repeat - each stage should move us a little closer to the \"true\" answer. \n",
    "    <ul>\n",
    "    <li> In other words, we are repeating the process over and over until we've found the solution that minimizes our overal cost/loss (the amount of error). \n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "This process is called Gradient Descent and is something we will care about more in ML class. Basically we define something called a loss function, which measures how much error we have. We then repeat a bunch of trials with different coef values, and measure the loss each time. We keep repeating until we've found the lowest amount of loss - or the smallest amount of error. The math can be complex, but the idea is pretty simple. If we manually changed the array of b values, ran the model, collected the LIKE value, and finally selected the combination with the best LIKE, that'd be a crude version of the same thing. This idea is common later on. Here, sklearn or statsmodels do it for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sigmoid and the Regression\n",
    "\n",
    "The sigmoid function is a function that takes in inputs (X values) and squishes all the outputs (Y values) between 0 and 1. The sigmoid is also the inverse of the logit function. The function is:\n",
    "\n",
    "$ g(x) = \\frac{1}{(1+e^-x)} = logit^-1 $\n",
    "\n",
    "A graph of what it ends up looking like is below. (Ignore the red line for now). The important part is now we have a way to connect the probabilities to our sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEvCAYAAABhSUTPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqQUlEQVR4nO3dd3RVVaLH8d9O7wkhIWBISCgBIqBACIp1VBQVZcYpdgVUxDbWGZ15b/RNeW+a03FERwWxYWEcsWLvqIRegwECCQkhhfSeu98fiRgwyIWUc8v3s1ZW7rn3JPfHuuTml33O2dtYawUAAICjE+B0AAAAAG9GmQIAAOgGyhQAAEA3UKYAAAC6gTIFAADQDZQpAACAbghy6okTEhJsWlqaU08PAADgtpUrV5ZZaxO7esyxMpWWlqacnBynnh4AAMBtxpidh3qMw3wAAADdQJkCAADoBsoUAABAN1CmAAAAuoEyBQAA0A2HLVPGmMeMMXuNMRsO8bgxxvzdGJNnjFlnjJnQ8zEBAAA8kzsjUwslTfuWx8+VNKLjY46kB7sfCwAAwDsctkxZaz+UVPEtu8yQtMi2+0xSnDFmUE8FBAAA8GQ9cc5UsqSCTtuFHfcBAAD4vJ6YAd10cZ/tckdj5qj9UKBSU1N74KkBAIAvanNZNba0qaGlTY37P1xqaGlTQ3PbAY8NTYzSpLR4x7L2RJkqlJTSaXuwpKKudrTWPizpYUnKysrqsnABAADv09jSpuqGFlU3tqquqVV1za2qa2pTXVOrapu+uq99+6v76pvbvn6sqbVTUXKpuc3l9nNfPjnV68vUUkk3G2MWS5osqcpaW9wD3xcAAPShNpdVZX2zKuqaVV7XrMr6FlU3tKiqoUXVje2fqxq+vq+qozxVNbSoudW98hMZEqjI0KCOj0BFhgQpKSZMkaFBiggOVHhIoMKCAxUeHKiw4ID921/d99X9YR37hgcHKjrMsaWGJblRpowxz0g6XVKCMaZQ0n2SgiXJWjtf0muSzpOUJ6le0qzeCgsAAI5MQ3Ob9tY0qqS6SXtrGlVe216UKuqa2ktTbXt5qqhr1r76ZrkOcdwowEgx4cGKCQtWbHj7x8DYMMWGBx9wf3RYkKK+Kksh7YUpKjRIER1lKSCgq7ODvNthy5S19tLDPG4l3dRjiQAAwGG5XFZltU0qrGxQSVWjSqobVVLTpJLqRu2tbv9cUt2o6sbWb3ytMVJceLDiI0PUPzJUwxKjNCk9RP0jQ9rviwpV/8gQxUUE7y9LUSFBPlmEeoKz42IAAKBLbS6rvTWNKtzXoN37GlS4r779dmXD/s8HH1oLCjBKignTgJj2gjRlWH8NiAnTwJgwJcWEKTE6VP2jQhQXHqygQBZB6SmUKQAAHGKtVWlNk7aX1Sm/rE47yur2395ZXv+Nk7ATokKUHBeuzEExOjszScn9wpUcF66Bse1lKT4ihNEjB1CmAADoZS6XVeG+BuWW1GhrSY1y99RoW2mt8svqVNfctn+/kMAADekfobSESJ0xaoBS4iM0uF+4BveLUHJcuMJDAh38V+BQKFMAAPSgfXXN2lBUpdw97aVpa0mNtpbUqqHl69I0uF94+3lKafEamhiptP6RSk+I1DFx4QpkZMnrUKYAADhKFXXNWr+7Sht2V2l9YZXW767S7sqG/Y8nRIVq5MAoXZKdopFJ0coYGK2MpGhFhfLr15fwagIA4IbmVpc2FFVp1c59Wrlzn9YVHlichvSP0PGpcbryxCEamxyrUQOj1T8q1MHE6CuUKQAAulBR16yVHcVp5c4KrS2s2n/1XEp8uManxumqjuJ0bHKsYsODHU4Mp1CmAACQVNXQos+3l+vTbeVavq1cuSU1kqTgQKMxybG66oQhykrrpwmp/TQgJszhtPAklCkAgF9qbGnTivwKfbqtXJ/mlWn97iq5rBQWHKBJafG68PhjNCktXuMGxyosmKvocGiUKQCA3yiqbNC7W/bqvS179cm2MjW2uBQUYDQ+NU43nzFCJw3rr+NT4xQaRHmC+yhTAACf1eayWr1rn97pKFBb9rQfukuJD9fFWSk6feQAZafHK5Kr69AN/O8BAPiU1jaXvsiv0Gvri/XGhhKV1TYpKMAoK62ffn7eKJ0xaoCGJUbJGOZzQs+gTAEAvF5Lm0vLt5Xr9Q179ObGPSqva1ZYcIDOGDVA08YM0mkZiVxth15DmQIAeCVrrdYVVunF1bu1dG2RKuqaFRkSqDNGJ+m8MQN12shERYTwaw69j/9lAACvUrivXi+tKdKSVYXaXlqnkKAATR2dpAuPP0anZSRy5R36HGUKAODxmlrbtGxjiZ75fJeWby+XJGWnxWvOKUN17thBHMKDoyhTAACPlV9Wp2e+2KXnVxaqoq5ZKfHhumNqhr43Plkp8RFOxwMkUaYAAB6mtc2ltzaV6MnPd+qTvHIFBhhNHZ2kyyan6uThCQoI4Co8eBbKFADAI1Q3tui5FQVa8Em+dlc2KDkuXHdOzdCPJqUoieVb4MEoUwAARxVU1GvBJ/l6LqdAtU2tyk6P170XZOqs0UkKZBQKXoAyBQBwxMaiKv3zvW16fUOxAozR+eMG6ZqT0zVucJzT0YAjQpkCAPSp1bv2ad67eXpny15FhwbpulOHauaUNA2KDXc6GnBUKFMAgD7x+fZyzXsvTx99Waa4iGDdMTVDV09JY1oDeD3KFACgV63etU9/eCNXy7eXKyEqRD87d5QuP2GIolhcGD6C/8kAgF6xtaRG9y/L1ZubSpQQFaJ7p2fqssmpzFAOn0OZAgD0qIKKev3l7a16cfVuRYUE6c6pGZp9croiGYmCj+J/NgCgR9Q0tmjeu3la8Em+jJGuO2WobjhtmPpFhjgdDehVlCkAQLe0uayezynQ/W/mqryuWT+YMFh3nJ3B1XnwG5QpAMBR+3x7uX758iZtKq5W1pB+WjAzW2MHxzodC+hTlCkAwBErqW7Ur17ZpFfXFSs5Llz/uHS8po8bJGOYsRz+hzIFAHBbm8vqieX5uv/NrWppc+n2szJ0/WlDuUIPfo0yBQBwy4bdVfr5i+u1rrBKp4xI0G++O0ZD+kc6HQtwHGUKAPCt6ppadf+buXr803zFR4bq75eO1wUc0gP2o0wBAA7p021l+ukL67S7skGXT07VT84ZxfIvwEEoUwCAb6hratXv39iiRct3Kq1/hJ6//kRlpcU7HQvwSJQpAMABPtterp+8sFaF+xo0+6R0/eSckQoP4QRz4FAoUwAASVJjS5t+/8YWLfgkX0P6R+jZOScqO53RKOBwKFMAAOXuqdGti1dry54azZySpp9OG6mIEH5FAO7gJwUA/Ji1Vk9+tlO/eXWzosOCtGDWJH1n5ACnYwFehTIFAH6qoq5ZP31hrd7evFenZSTq/h8ep8ToUKdjAV6HMgUAfmj5tnLduni1KutbdO/0TM2ckqaAAOaNAo5GgDs7GWOmGWNyjTF5xph7ung81hjzsjFmrTFmozFmVs9HBQB0l7VWD76/TZc/8pmiwoL04k1TNPvkdIoU0A2HHZkyxgRKekDSVEmFklYYY5Zaazd12u0mSZustRcYYxIl5RpjnrLWNvdKagDAEatqaNGdz63V25tLdP64Qfr998cpKpQDFEB3ufNTlC0pz1q7XZKMMYslzZDUuUxZSdGmfW2BKEkVklp7OCsA4ChtLKrSDU+uUlFlg+6dnqlZJ6WxHAzQQ9wpU8mSCjptF0qafNA+8yQtlVQkKVrSxdZaV48kBAB0y3M5BfrFfzaoX0SInr3+BE0cwtxRQE9yp0x19aeLPWj7HElrJJ0haZikt4wxH1lrqw/4RsbMkTRHklJTU484LADAfa1tLv3m1c1a+Gm+ThreX3+7ZLwSorhaD+hp7pyAXigppdP2YLWPQHU2S9K/bbs8STskjTr4G1lrH7bWZllrsxITE482MwDgMKrqWzRr4Qot/DRfs09K1+OzsilSQC9xZ2RqhaQRxph0SbslXSLpsoP22SXpTEkfGWOSJI2UtL0ngwIA3JO3t1bXPr5Cuysb9Ifvj9OPJqUc/osAHLXDlilrbasx5mZJyyQFSnrMWrvRGDO34/H5kn4taaExZr3aDwveba0t68XcAIAuvJe7Vz9+erVCgwP0zHUnKCuN86OA3ubWNbHW2tckvXbQffM73S6SdHbPRgMAHIkFn+zQr1/ZpFEDY/Svq7OUHBfudCTALzDBCAB4uTaX1f++ulmPfbJD5xybpL9cfDyLFAN9iJ82APBijS1tum3xGr2xcY9mn5Su/zp/tAKZzRzoU5QpAPBS5bVNunZRjtYUVOre6ZmafXK605EAv0SZAgAvtKOsTjMXfKE9VY168PIJmjZmkNORAL9FmQIAL7O2oFKzFq6QJD193QmaOKSfw4kA/0aZAgAv8mlema5blKP4qBAtmj1Z6QmRTkcC/B5lCgC8xLKNe3TL06uVnhCpRddkKykmzOlIAESZAgCv8FxOge5Zsk7HpcRpwcxJiosIcToSgA6UKQDwcI98tF2/eXWzThmRoIeunMgcUoCH4ScSADyUtVZ/enOr5r2Xp/PHDtKfLz5OoUGBTscCcBDKFAB4IGvbZzV/5OMdujQ7Rb/57lgm4wQ8FGUKADyMtVa/fHmTFn6ar5lT0nTfBZkyhiIFeCrKFAB4EJfL6t6lG/TkZ7t07cnty8NQpADPRpkCAA/hcln9/MX1WryiQHNPG6a7p42kSAFegDIFAB6gzWV195J1emFloW45Y7jumJpBkQK8BGUKABzW5rK66/m1enH1bt1+VoZuPWuE05EAHAHKFAA4yOWyumfJOr24erfuOjtDN59BkQK8TYDTAQDAX1lr9YuXNuj5lYW67awRFCnAS1GmAMAB1lr9+pXNeurzXZp72jDdeiZFCvBWlCkA6GPWWv1hWa4e+2SHZp2UxlV7gJejTAFAH/v7O3l68P1tunxyqu6dzoScgLejTAFAH5r/wTb95e2t+sHEwfr1jDEUKcAHUKYAoI88sTxfv3t9iy487hj9/vvjFMBae4BPoEwBQB94ac1u3bt0o84anaQ//eg4Fi0GfAhlCgB62fu5e3Xnc2uVnRaveZeNV3Agb72AL+EnGgB60cqdFZr75EqNHBitf12dpbDgQKcjAehhlCkA6CVb9lRr1oIVGhQbrsdnZysmLNjpSAB6AWUKAHpBQUW9rnr0C4WHBGrR7GwlRIU6HQlAL2FtPgDoYaU1Tbri0c/V1OrS83NPVEp8hNORAPQiRqYAoAfVNLbo6se+0N7qJi2YNUkZSdFORwLQyyhTANBDmltduuHJVdpaUqMHr5igCan9nI4EoA9wmA8AeoC1VvcsWaeP88p0/w+P0+kjBzgdCUAfYWQKAHrAn97cqn+v3q07p2boBxMHOx0HQB+iTAFANz31+U7Ney9Pl2an6OYzhjsdB0Afo0wBQDe8s7lEv/jPBn1nZCILFwN+ijIFAEdpbUGlbn56tY49JlbzLpugIJaJAfwSP/kAcBR2ltdp9sIVSogO0WMzJykylOt5AH9FmQKAI1RR16yZC1aozVotnJWtxGhmNwf8GWUKAI5AU2ub5izKUVFlgx69OkvDEqOcjgTAYYxLA4Cb2ueSWq+cnfv0wGUTNHFIvNORAHgARqYAwE3/eDdPL67erbvOztD54wY5HQeAh3CrTBljphljco0xecaYew6xz+nGmDXGmI3GmA96NiYAOOvltUX681tbddGEZN30HeaSAvC1wx7mM8YESnpA0lRJhZJWGGOWWms3ddonTtI/JU2z1u4yxrCOAgCfsWrXPt35/Fplp8XrtxeNZS4pAAdwZ2QqW1KetXa7tbZZ0mJJMw7a5zJJ/7bW7pIka+3eno0JAM4oqKjXnEU5GhQbpvlXTlRoUKDTkQB4GHfKVLKkgk7bhR33dZYhqZ8x5n1jzEpjzFU9FRAAnFLd2KJrHl+h5laXHr16kuIjQ5yOBMADuXM1X1fj2baL7zNR0pmSwiUtN8Z8Zq3desA3MmaOpDmSlJqaeuRpAaCPtLa5dPPTq7W9tE6Pz87W8AFMgQCga+6MTBVKSum0PVhSURf7vGGtrbPWlkn6UNJxB38ja+3D1tosa21WYmLi0WYGgF73q1c26cOtpfrNd8fopOEJTscB4MHcKVMrJI0wxqQbY0IkXSJp6UH7vCTpFGNMkDEmQtJkSZt7NioA9I2Fn+zQouU7NefUobokm1F0AN/usIf5rLWtxpibJS2TFCjpMWvtRmPM3I7H51trNxtj3pC0TpJL0iPW2g29GRwAesMHW0v1q1c2aWpmku6eNsrpOAC8gLH24NOf+kZWVpbNyclx5LkBoCvbS2s144FPlBwXriU3TGHxYgD7GWNWWmuzunqMGdABQO1X7l27KEfBgQH611VZFCkAbqNMAfB7bS6rW59ZrV3l9frn5ROUEh/hdCQAXoQyBcDv/XFZrt7LLdV9Fx6rE4b2dzoOAC9DmQLg115as1vzP9imyyen6soThjgdB4AXokwB8FvrCiv10xfWKTs9XvddcKzTcQB4KcoUAL+0t6ZRcxatVEJUqB68fIJCgng7BHB0uFwFgN9pam3T3CdWqqqhRUtumKL+UaFORwLgxShTAPyKtVb//eIGrdpVqX9ePkGZx8Q4HQmAl2NcG4BfWfBJvp5fWagfnzFc540d5HQcAD6AMgXAb3z8ZZn+97XNOjszSbedleF0HAA+gjIFwC/kl9XppqdXaVhipP588fEKCDBORwLgIyhTAHxeTWOLrluUI2OkR66apCiWigHQg3hHAeDTXC6r259do+1ldXpidrZS+7NUDICexcgUAJ/257e26u3Ne/WL80dryvAEp+MA8EGUKQA+65V1RZr3Xp4umZSiq6ekOR0HgI+iTAHwSRt2V+mu59cqa0g//WrGGBnDCecAegdlCoDPKatt0pxFOYqPCNGDV0xkqRgAvYoT0AH4lOZWl254cqUq6pv1wtwpSoxmqRgAvYsyBcBnWGt139KNWpG/T/+4dLzGJMc6HQmAH3CuTOXmSqef7tjTA/A9JdWNmlFWpxviwpX6GVMgAOgbnEgAwCdUNbQov7xecREhSomnSAHoO86NTI0cKb3/vmNPD8B3FFTU68J5H6t/VKhevHGKTFiw05EA+JpvuSKYkSkAXq2uqVXXLcpRm8vqX1dlKZoiBaCPcQI6AK/lclnd8dwabS2p0cJZ2UpPiHQ6EgA/xMgUAK/1t3e+1LKNJfr5eaN1akai03EA+CnKFACv9Pr6Yv3tnS/1g4mDdc3J6U7HAeDHKFMAvM6momrd8dxajU+N0/9+j6ViADiLMgXAq5TXNum6RTmKDQ/WQ1dMVGhQoNORAPg5TkAH4DWaW1264alVKqtt0vNzT9SAmDCnIwEAZQqA9/jlyxv1xY4K/e2S4zVucJzTcQBAEof5AHiJJz7bqac+36W5pw3TjOOTnY4DAPtRpgB4vM+2l+uXSzfqjFED9JNzRjodBwAOQJkC4NEKKup141OrNKR/hP56yfEKDODKPQCehTIFwGN9tVRMa5tLj1w9STEsFQPAA3ECOgCP5HJZ3fncWpaKAeDxGJkC4JH+/u6XemPjHpaKAeDxKFMAPM7r64v117dZKgaAd6BMAfAom4urdefzLBUDwHtQpgB4jNKaJl37eI5iwlgqBoD34AR0AB6hsaVN1z+Ro/K6Jj1//RSWigHgNShTABxnrdXdS9Zp1a5Kzb9igsYOjnU6EgC4za3DfMaYacaYXGNMnjHmnm/Zb5Ixps0Y84OeiwjA1817N08vrSnST84ZqWljBjkdBwCOyGHLlDEmUNIDks6VlCnpUmNM5iH2+72kZT0dEoDvenVdsf701lZdNCFZN54+zOk4AHDE3BmZypaUZ63dbq1tlrRY0owu9rtF0hJJe3swHwAftragUnc8t0ZZQ/rptxeN5co9AF7JnTKVLKmg03Zhx337GWOSJX1P0vyeiwbAlxVXNei6RTkaEBOqh67kyj0A3sudMtXVn4r2oO2/SrrbWtv2rd/ImDnGmBxjTE5paambEQH4mvrmVl37eI4amtv06NWT1D8q1OlIAHDU3Lmar1BSSqftwZKKDtonS9LijiH6BEnnGWNarbX/6byTtfZhSQ9LUlZW1sGFDIAfcLmsblu8RpuLq/XYzEnKSIp2OhIAdIs7ZWqFpBHGmHRJuyVdIumyzjtYa/ev92CMWSjplYOLFABI0h+W5erNTSW674JMnT5ygNNxAKDbDlumrLWtxpib1X6VXqCkx6y1G40xczse5zwpAG55bkWB5n+wTVeckKqZU9KcjgMAPcKtSTutta9Jeu2g+7osUdbamd2PBcDXfLi1VD97cb1OzUjUfRccy5V7AHwGa/MB6HWbiqp141OrlJEUrX9ePkHBgbz1APAdvKMB6FXFVQ2avXCFokKDtGDmJEWFsooVAN/CuxqAXlPT2KJZC1aotqlVz889UQNjWbwYgO+hTAHoFS1tLt341Cp9ubdWC2ZO0uhBMU5HAoBewWE+AD3OWqv/fnGDPvqyTL/93lidmpHodCQA6DWUKQA97p/vb9OzOQW65Yzh+tGklMN/AQB4McoUgB71n9W79cdlufre+GTdMTXD6TgA0OsoUwB6zAdbS3XX82t1wtB4/e77Y5lLCoBfoEwB6BFrCip1w5MrlZEUrYevylJoUKDTkQCgT1CmAHTbttJazV64Qv2jQrRw9iTFhAU7HQkA+gxlCkC3lFQ36qpHv5CR9MTsyRoQzVxSAPwLZQrAUatqaNFVj36hyvpmLZyVrbSESKcjAUCfY9JOAEelsaVN1z2eo+1ltVowM1tjB8c6HQkAHEGZAnDEWttc+vEzq7ViZ4X+cel4nTwiwelIAOAYDvMBOCIul9XdS9brzU0lum96pqaPO8bpSADgKMoUALdZa/U/L2/UklWFuv2sDM08Kd3pSADgOMoUALf9YVmuFi3fqTmnDtWPzxzudBwA8AiUKQBueeC9PD34/jZdPjlVPzt3FLObA0AHyhSAw1rwyY796+39esYYihQAdEKZAvCtnltRoF++vEnnHJukP/5gnAICKFIA0BllCsAhLV1bpHv+vU6nZiTq75eOV1AgbxkAcDDeGQF06ZV1Rbr92TXKSovXQ1dMZOFiADgEyhSAb3h1XbFuXbxGE1P7acHMSQoPoUgBwKFQpgAc4NV1xfrx4tWakBqnBbMmKTKUhRIA4NtQpgDsd2CRyqZIAYAbKFMAJEmvrW8vUuNT2otUFEUKANxCmQKg19YX65Zn2ovUwtkUKQA4EpQpwM+9sLJQNz+9iiIFAEeJd03Ajz2xPF+/eGmjTh6eoIevmqiIEN4SAOBI8c4J+Kn5H2zT717forNGJ2neZeMVFsz0BwBwNChTgJ+x1uovb23V39/N0wXHHaM//+g4BTOzOQAcNcoU4EestfrNq5v16Mc7dHFWiv7vorEKZK09AOgWyhTgJ1raXLpnyXotWVWomVPSdO/0TBYtBoAeQJkC/EB9c6tufGqV3s8t1W1njdCtZ46QMRQpAOgJlCnAx5XXNmn24zlaX1ip3140VpdmpzodCQB8CmUK8GEFFfW66rEvVFTZoIeuzNLUzCSnIwGAz6FMAT5qY1GVZi5YoeZWl566drKy0uKdjgQAPokyBfig93L36panVysmLEhPzz1RI5KinY4EAD6LMgX4mIWf7NCvXtmk0YNi9OjVkzQwNszpSADg0yhTgI9obXPpV69s0qLlOzU1M0l/vfh4RbLOHgD0Ot5pAR9Q3diiW55erQ+2lmrOqUN197RRTMYJAH3ErTUkjDHTjDG5xpg8Y8w9XTx+uTFmXcfHp8aY43o+KoCuFFTU6wcPfqpP8sr0u4vG6ufnjaZIAUAfOuzIlDEmUNIDkqZKKpS0whiz1Fq7qdNuOySdZq3dZ4w5V9LDkib3RmAAX/voy1Ld8sxquVxWi2Zna8rwBKcjAYDfcecwX7akPGvtdkkyxiyWNEPS/jJlrf200/6fSRrckyEBHMhaq4c+3K4/vLFFIwZE66ErJyotIdLpWADgl9wpU8mSCjptF+rbR52ukfR6d0IBOLS6plb9dMk6vbquWOePG6Q/fH8cJ5oDgIPceQfu6uQL2+WOxnxH7WXq5EM8PkfSHElKTWVJC+BI5ZfV6fonVurLvTX62bmjNOfUoayxBwAOc6dMFUpK6bQ9WFLRwTsZY8ZJekTSudba8q6+kbX2YbWfT6WsrKwuCxmArr2xoVg/eWGdAgOMHp+drVNGJDodCQAg98rUCkkjjDHpknZLukTSZZ13MMakSvq3pCuttVt7PCXgx5pa2/R/r27W48t36rjBsZp32QSlxEc4HQsA0OGwZcpa22qMuVnSMkmBkh6z1m40xszteHy+pHsl9Zf0z45DDq3W2qzeiw34h/yyOt38zCpt2F2t2Sel655zRykkyK0ZTQAAfcRY68zRtqysLJuTk+PIcwPe4JV1RbpnyXoFBhjd/8PjNDUzyelIAOC3jDErDzVQxCVAgIepbWrVr1/epGdzCjQ+NU7/uHS8BvfjsB4AeCrKFOBBcvIrdPtza7R7X4NuPH2Ybp+aoeBADusBgCejTAEeoLnVpb++vVXzP9im5H7hevb6EzUpLd7pWAAAN1CmAIdtLanRbYvXaFNxtS7OStEvLshUFJNwAoDX4B0bcEhzq0vzP9imee/mKTosSP+6KouTzAHAC1GmAAesLajU3UvWacueGk0fN0j/c+GxSogKdToWAOAoUKaAPtTQ3KY/v5WrRz/eocToUEajAMAHUKaAPvLB1lLd+9IG7Syv12WTU3XPuaMUExbsdCwAQDdRpoBeVrivXr9+ZZOWbSxRekKknrnuBJ04rL/TsQAAPYQyBfSSptY2/evD7Zr3Xp6MjH5yzkhde0q6QoMCnY4GAOhBlCmgh1lr9X5uqX758kbll9fr3DED9d/TM5UcF+50NABAL6BMAT1oY1GVfvvaFn2cV6ahCZF64ppsnTIi0elYAIBeRJkCekBxVYPuX7ZV/15dqNjwYN13QaYunzxEIUEsBQMAvo4yBXRDdWOLHv5gux75eLtcVppz6lDdePpwxYZzlR4A+AvKFHAUapta9fin+Xr4w+2qamjRjOOP0V1nj1RKfITT0QAAfYwyBRyB+uZWPbF8px76cLsq6pp15qgBun1qhsYkxzodDQDgEMoU4Ib65lY9/fkuzf9gm8pqm3VaRqJun5qh41PinI4GAHAYZQr4FhV1zVr4ab4WLc9XZX2LThreX/PPylBWWrzT0QAAHoIyBXShcF+9Hvlohxav2KXGFpemZiZp7mnDNHFIP6ejAQA8DGUK6GT1rn1a+Gm+XllXLCPpu+OTdf2pQzUiKdrpaAAAD0WZgt9rbGnTq+uKtWh5vtYWVikqNEgzp6TpmpPTdQyzlgMADoMyBb9VVNmgpz7fqcVfFKi8rlnDEiP1qxnH6qIJgxUVyo8GAMA9/MaAX2lqbdM7m/fq2RUF+ujLUknSmaOTdPWJaTppeH8ZYxxOCADwNpQp+IUte6r13IpCvbi6UPvqWzQoNkw3fWe4fpSVwkSbAIBuoUzBZ5VUN+rltUVaurZI6wqrFBxodHbmQP0wa7BOGZGowABGoQAA3UeZgk/ZV9es1zfs0dK1u/X5jgpZKx17TIzunZ6p745PVnxkiNMRAQA+hjIFr1dR16x3Npfo9Q179OHWUrW6rIYmRurWM0foguOO0bDEKKcjAgB8GGUKXmlneZ3e2lSiNzeVKCe/Qi4rJceF65qT03XBccfo2GNiOJkcANAnKFPwCi1tLq0pqNQHuaV6a1OJcktqJEmjBkbr5jNG6OzMJAoUAMARlCl4rIKKen2wtVQfbi3V8m3lqmlqVYCRJqXF6xfTM3V2ZhJX4gEAHEeZgscorWnSFzsq9Nn2cn30Zanyy+sltR++m37cIJ06IlFThiUoNiLY4aQAAHyNMgXHFFU26PMd5fpiR4U+31Gh7aV1kqTw4ECdMDReV09J06kZiRqaEMnhOwCAx6JMoU80trRpU3G11uyq1NrCSq3cuU+F+xokSdFhQZqUFq+Ls1KUnR6vMcmxCg4McDgxAADuoUyhx7W5rHaU1WltQaXWFLSXp83F1Wpps5KkpJhQjU/pp9knpWvy0HiNGhjDBJoAAK9FmUK31Da1aktxtTYXV2tTcY02F1crd0+NGlraJEmRIYEaNzhO15w8VMenxOn4lDgNjA1zODUAAD2HMgW31De3antpnbaV1mrb3lrlltRoc3GNdlXU798nNjxYowdF65LsFI0eFKPjU+I0LDGKUScAgE+jTGE/l8tqb02TdpR1lKbSWuXtrdX20jrtrmzYv1+AkYb0j9SY5Bj9cOJgjR4Uo8xjYjQoNowTxQEAfocy5Wdqm1pVUFGvXRX1+z9/9VG4r0HNra79+4YHB2rYgEhNSuunSxJTNGxAlIYlRiktIUKhQYEO/isAAPAclCkf0tDcpuKqBu2palRxVaP2VDcesF1c1aiKuuYDviY6NEip/SM0MilaU0e3T4KZGh+h4QOiNDAmTAEcogMA4FtRpjxca5tLFXXNKq1tUllts8prm1TWcfurz3ur24tSVUPLN74+LiJYA2PCNCg2TOMGxyklPlypHYUpNT5CseHBHJoDAKAbKFN9qLGlTZX1LapsaG7/XN+iqq9uN3y9va+upaMoNWlf/TcLkiSFBAUoMSpUCVEhGtwvXJPS4jUwtr00tX8O18CYMIWHcDgOAIDe5FaZMsZMk/Q3SYGSHrHW/u6gx03H4+dJqpc001q7qoezOqa1zaW6pjbVNLWotqlVtY2t7Z+/7XbHdk1j6/7y1NTpfKSDBQUYxUWEKC4iWHHhwRqWGKXs9HglRIUqITpUiVEh6h8V2r4dFaKo0CBGlAAA8ACHLVPGmEBJD0iaKqlQ0gpjzFJr7aZOu50raUTHx2RJD3Z8dkxVQ4u+2FGhhpY2NTa3qaGl46O5TY2dbje0HLztat/u9DXN31KCOosMCVRkaJCiwoIUHRqkyNAgpSVEKC48TnERwYqNCFZc+NeFKTYiuL1AhQcrIiSQcgQAgBdyZ2QqW1KetXa7JBljFkuaIalzmZohaZG11kr6zBgTZ4wZZK0t7vHEbircV6/rFuV0+VhYcIDCgwMVHhyosJDA/bcjQ4PUP+rr7fCQQIUFByoiJFBRB5Wkg29HhgQxnxIAAH7InTKVLKmg03ahvjnq1NU+yZIcK1NDE6L0yi0nK6yjFH1VkEKDArhCDQAA9Bh3ylRXzcMexT4yxsyRNEeSUlNT3XjqoxceEqgxybG9+hwAAAABbuxTKCml0/ZgSUVHsY+stQ9ba7OstVmJiYlHmhUAAMDjuFOmVkgaYYxJN8aESLpE0tKD9lkq6SrT7gRJVU6eLwUAANBXDnuYz1rbaoy5WdIytU+N8Ji1dqMxZm7H4/Mlvab2aRHy1D41wqzeiwwAAOA53Jpnylr7mtoLU+f75ne6bSXd1LPRAAAAPJ87h/kAAABwCJQpAACAbqBMAQAAdANlCgAAoBsoUwAAAN1AmQIAAOgG0z6rgQNPbEyppJ2OPLl3SpBU5nQIfAOvi+fhNfFMvC6eh9fkyAyx1na5fItjZQpHxhiTY63NcjoHDsTr4nl4TTwTr4vn4TXpORzmAwAA6AbKFAAAQDdQprzHw04HQJd4XTwPr4ln4nXxPLwmPYRzpgAAALqBkSkAAIBuoEx5IWPMXcYYa4xJcDqLvzPG/NEYs8UYs84Y86IxJs7pTP7MGDPNGJNrjMkzxtzjdB5/Z4xJMca8Z4zZbIzZaIy51elMaGeMCTTGrDbGvOJ0Fl9AmfIyxpgUSVMl7XI6CyRJb0kaY60dJ2mrpJ85nMdvGWMCJT0g6VxJmZIuNcZkOpvK77VKutNaO1rSCZJu4jXxGLdK2ux0CF9BmfI+f5H0U0mc7OYBrLVvWmtbOzY/kzTYyTx+LltSnrV2u7W2WdJiSTMczuTXrLXF1tpVHbdr1P7LO9nZVDDGDJZ0vqRHnM7iKyhTXsQYc6Gk3dbatU5nQZdmS3rd6RB+LFlSQaftQvGL22MYY9IkjZf0ucNRIP1V7X+UuxzO4TOCnA6AAxlj3pY0sIuH/kvSzyWd3beJ8G2vibX2pY59/kvthzSe6stsOIDp4j5GcD2AMSZK0hJJt1lrq53O48+MMdMl7bXWrjTGnO5wHJ9BmfIw1tqzurrfGDNWUrqktcYYqf1w0ipjTLa1dk8fRvQ7h3pNvmKMuVrSdElnWuYacVKhpJRO24MlFTmUBR2MMcFqL1JPWWv/7XQe6CRJFxpjzpMUJinGGPOktfYKh3N5NeaZ8lLGmHxJWdZaFql0kDFmmqQ/SzrNWlvqdB5/ZowJUvtFAGdK2i1phaTLrLUbHQ3mx0z7X36PS6qw1t7mcBwcpGNk6i5r7XSHo3g9zpkCumeepGhJbxlj1hhj5jsdyF91XAhws6Rlaj/R+TmKlONOknSlpDM6fj7WdIyIAD6FkSkAAIBuYGQKAACgGyhTAAAA3UCZAgAA6AbKFAAAQDdQpgAAALqBMgUAANANlCkAAIBuoEwBAAB0w/8DfSdj9DJJbLEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating vectors X and Y\n",
    "x = np.linspace(-5, 5, 10000)\n",
    "y = sigmoid(x)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "# Create the plot\n",
    "plt.plot(x, y)\n",
    "# Show the plot\n",
    "plt.axhline(.5, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression is going to use this sigmoid functions to generate a prediction between 0 and 1. Can can plug the linear regression equation into the sigmoid function, then our new hypothesis becomes:\n",
    "\n",
    "$ y = \\frac{1}{(1+e^(m*x+b))} $\n",
    "\n",
    "Where y = probability. \n",
    "\n",
    "Note - the mathmatical derivations aren't really super-duper critical. If it is confusing, just ingore it. There's a full derivation and example here: https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Real Example - SciKitLearn</h2>\n",
    "\n",
    "For our first try we can use the diabetes example we've used a bit before, though we've always sidesteped the true target. The outcome value is whether or not someone is diabetic, and all the other variables that are risk factors that we can use to predict if someone will become diabetic. Our aim is to predict, yes or no, will someone develop diabetes based on those risk factors. \n",
    "\n",
    "First - one variable. We'll use BMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 1) (768, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df[\"Outcome\"]).reshape(-1,1)\n",
    "x = np.array(df[\"BMI\"]).reshape(-1,1)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>ravel()</b> - sometimes you may get a message that says something like \"we want the y data in the shape (samples,)\". This is obviously a contradiction to what we said to always shape the y as (samples, 1). The easiest way to deal with this is to use the .ravel() function as shown below. The cause is the expectation of data format for whatever you're using, which can vary. If we always make the y array (samples, 1) and then use ravel when needed, that allows us to be consistent and not worry about it much. I'd suggest keeping with this for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6220472440944882\n"
     ]
    }
   ],
   "source": [
    "md1 = LogisticRegression().fit(X_train,y_train.ravel())\n",
    "md1Pred = md1.predict(X_test)\n",
    "\n",
    "score = md1.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66% accuracy. Not bad. We can try with more Xs though...\n",
    "\n",
    "#### Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all the X values. \n",
    "# I can use the y from above still \n",
    "df2 = df.drop(columns={\"Outcome\"})\n",
    "x2 = np.array(df2)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7480314960629921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akeems/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#I'm reusing some varaible names to make my life easier with copy/paste. \n",
    "#Make sure you run things in order if you do this. \n",
    "X_train, X_test, y_train, y_test = train_test_split(x2, y, test_size=0.33)\n",
    "\n",
    "md2 = LogisticRegression().fit(X_train,y_train.ravel())\n",
    "md2Pred = md2.predict(X_test)\n",
    "score = md2.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this the first time, I didn't get an answer, I instead got something along the lines of \"failed to converge\". This means that the gradient descent process didn't finish, and the algorithm didn't settle on an answer. We will explore this more in the machine learning stuff, for now we can just tell it to set a higher cap on how long it can run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7480314960629921\n"
     ]
    }
   ],
   "source": [
    "md2 = LogisticRegression(max_iter=1000).fit(X_train,y_train.ravel())\n",
    "md2Pred = md2.predict(X_test)\n",
    "score = md2.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results \n",
    "\n",
    "We can demonstrate some results... We'll look into result details more later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix \n",
    "preds = md2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81       165\n",
      "           1       0.66      0.58      0.62        89\n",
      "\n",
      "    accuracy                           0.75       254\n",
      "   macro avg       0.72      0.71      0.72       254\n",
      "weighted avg       0.74      0.75      0.74       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, preds, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138  27]\n",
      " [ 37  52]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, preds), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.80314960629921\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, preds)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Work Through Titanic</h1>\n",
    "\n",
    "Predict who lives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data\n",
    "dfe = pd.read_csv(\"data/train.csv\")\n",
    "dfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe[\"no_cabin\"] = dfe[\"Cabin\"].isnull()\n",
    "dfe[\"family\"] = dfe[\"SibSp\"] + dfe[\"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dropped age due to missing values. Think about if there's anything else we may want to do to it instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>no_cabin</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex     Fare Embarked  no_cabin  family\n",
       "0         0       3    male   7.2500        S      True       1\n",
       "1         1       1  female  71.2833        C     False       1\n",
       "2         1       3  female   7.9250        S      True       0\n",
       "3         1       1  female  53.1000        S     False       1\n",
       "4         0       3    male   8.0500        S      True       0"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfe2 = dfe.drop(columns={\"Name\", \"Ticket\", \"Cabin\", \"SibSp\", \"Parch\", \"PassengerId\", \"Age\"})\n",
    "dfe2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 7), (891, 1))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Redo the dummy variables. \n",
    "dfe2_dumb = pd.get_dummies(dfe2, drop_first=True)\n",
    "\n",
    "ye = np.array(dfe2_dumb[\"Survived\"]).reshape(-1,1)\n",
    "xe = dfe2_dumb.drop(columns={\"Survived\"})\n",
    "xe.shape, ye.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8116591928251121"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainT, X_testT, y_trainT, y_testT = train_test_split(xe, ye)\n",
    "\n",
    "titan = LogisticRegression(max_iter=1000).fit(X_trainT,y_trainT.ravel())\n",
    "titan_preds = titan.predict(X_testT)\n",
    "scoreT = titan.score(X_testT, y_testT)\n",
    "scoreT"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
