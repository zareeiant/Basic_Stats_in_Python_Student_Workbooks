{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinkplot\n",
    "import thinkstats2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "import random\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "\n",
    "Doing a simple linear regression is cool, but for this to be really useful we want to be able to use a bunch of factors to make a prediction. The process is largely the same, we just have more variables on the input (X) side. It is also difficult/impossible to visualize, since we'd need to see in 3+ dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "First, we'll get some data, and look at what the target and the features are going to be. This is a really simplifed example of some EDA that we normally do in advance of machine learning. The first step to any predictive model building is to look at our data and see if anything should be done prior to modelling. This is pretty vague, but we are basically looking to see if there's anything in the data that might be suboptimal for the predictions we want to create. Some common things to look for are:\n",
    "<ul>\n",
    "<li> Errors - anything that is just \"wrong\", words in numeric columns, mislabeled values, etc...\n",
    "<li> Outliers - values too large or too small. \n",
    "<li> Weird Distributions - this is very open-ended, but think of something like a numerical feature where the variance is exceeding low. If every row of the data has the same value, it probably won't help in making predictions, which is inheirently seeking to discriminate. For example, some schools in Japan used to force students to dye their hair black if it wasn't already; if hair color was a feature in your dataset, it wouldn't really add any value if 99.9% of rows of data is \"black\". \n",
    "<li> Missing Values - how should we deal with it if some values are blank?\n",
    "</ul>\n",
    "\n",
    "This isn't a conclusive list, and as we explore we may find other things to change or remove. The amount of work we may need to do can also vary widely - if the data is comming from something like a PoS system it will probably be pretty clean and not require many changes; if the data is coming from scraping social media sites from that people fill out, the data may well require massive amounts of cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "#I'll drop density to make it more realistic.\n",
    "df = pd.read_csv(\"data/bodyfat.csv\")\n",
    "df = df.drop(columns={\"Density\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. We'll predict the BodyFat (which is hard to accurately measure) by using the other stuff, which is easier to measure. \n",
    "\n",
    "Before we start, we want to do some exploration of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Exploring the Data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, it looks like there aren't a lot of issues. One thing I notice is that there's a BodyFat reading of 0 for the min. That isn't possible, so we'll need to get rid of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get variable info. \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our variables are correctly identified as numbers. Often if there's some erroneous data in a column it might get miscategorized as an object. Since we'll be dealing with non-numeric data in ML, starting next workbook, we probably want to double check.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual correlation and distributions\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears there are some outliers that probably aren't helping - e.g. Height. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are there nulls?\n",
    "df.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some outliers\n",
    "# These were judgement cals. \n",
    "# E.g. it is theoretically possible for someone to have fat < 5%, but that's really only people like\n",
    "# bodybuilders at a competition time. Those types of extreme outliers probably wont' help our model. \n",
    "df = df[df[\"BodyFat\"] > 5]\n",
    "df = df[df[\"Height\"] > 40]\n",
    "df = df[df[\"Weight\"] < 300]\n",
    "df = df[df[\"Ankle\"] < 30]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine Distributions Post-Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check with cleaned data. \n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good. We may want to examine some of the outliers a bit more closely, but overall the data looks pretty usable. Distributions are all pretty normal. Large outliers are gone. Nothing really jumps out as being a problem.\n",
    "\n",
    "There does appear to be a large amount of correlation between variables. We'll worry about that later on, for now, looks good. Time to start regressing...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the data\n",
    "\n",
    "The basic idea here is the same as one variable regression, only instead of one X, we have a bunch. We can't do this with simple functions (thinkstats), at least not easily, so we'll use a package. \n",
    "\n",
    "### SciKitLearn Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y data is simple, do that first\n",
    "y = np.array(df[\"BodyFat\"]).reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y is identical to the single variable stuff. For X, we have a width to our array. We can take a look again to see what we expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a new df with only the features we'll use\n",
    "df_ = df.drop(columns={\"BodyFat\"})\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make that df into an array. \n",
    "x = np.array(df_)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapes\n",
    "print(\"X shape\", x.shape)\n",
    "print(\"Y shape\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKL Regress...\n",
    "\n",
    "Shape for this one looks good! We have 240 rows, which matches the Y. We have 13 columns, which is what we get if we were to count up the columns by hand. Success!!\n",
    "\n",
    "Time for regression stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate model \n",
    "model = LinearRegression().fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get some info on our new regression model\n",
    "r_sq = model.score(xTest, yTest)\n",
    "print('R-squared:', r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View \"Slopes\" and Intercept\n",
    "\n",
    "In single regression our model was made up of the slope, and intercept values - we can plug any X in and calculate our prediction. In multiple regression, the results are the same, there's just more of them. We still have one intercept, but now each X has it's own m. The entire equation ends up just having a bunch of \"X\" terms:\n",
    "\n",
    "$ y = m1 * x1 + m2 * x2 + m3 * x3 ... + b $\n",
    "\n",
    "We can extract the values just as we did before, we could also make a manual model to generate predictions - our equation would just be longer than the y = mx + b ones that we created earlier. \n",
    "\n",
    "#### Visualizing Multiple Regression\n",
    "\n",
    "The idea of how multiple regression works is the same, it is just harder to visualize. If we try to look at an example with 2 features (so 3 total values, with the target) and picture it a little. In a single regression we make a prediction by doing a \"lookup\" in one dimension - the X value is our input, we slide along the X axis until we find that value, then look up to the regression line to see what the target is at that point. In a 2 feature regression, the same idea applies, only we do the lookup with two X values on a plane and the prediction is where they intersect on the 3rd axis:\n",
    "\n",
    "![3D Regression](images/3d_regression.png \"3D Regression\")\n",
    "\n",
    "Now our model, or our line of best fit, is a plane - a two dimensional line. Each feature we add ups that number of dimensions by 1, and it is always 1 less than the total number of dimensions. In a single regression teh value on one axis predicts the other axis, in a multiple regression the value on [# of features] axis, where they interesect, predicts the value on the other axis. Same, same. \n",
    "\n",
    "Going up in features, the same idea always applies, we just don't have a timecube to draw it. If we had 13 features like we do above, we'd have a scatter plot in 14 dimensional space, and we'd look to the intercept of the 13 features, which would give us our prediction on the 14th axis - the one belonging to the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our coefficent/slope is now an array of values - one per X. \n",
    "#Visualizing the regression would be a 14D space, where these are the slopes in each dimension. \n",
    "print('Intercept:', model.intercept_[0])\n",
    "print('Coefs:', model.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error Metrics\n",
    "\n",
    "Get the residuals to calculate RMSE. \n",
    "\n",
    "The residual stuff is the same no matter how many Xs we have on the input side, since all we are checking is the Y values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get RMSE\n",
    "tmp = model.predict(xTest)\n",
    "mean_squared_error(tmp, yTest, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Residuals\n",
    "\n",
    "We don't generally need to view the residuals if we aren't doing things by hand, the library functions allow us to just look at the summary statistics like RMSE. We can view them for fun, just to confirm they exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Residuals and picture them in a DF for easy reading. \n",
    "tmp1 = pd.DataFrame(yTest, columns={\"Y values\"})\n",
    "tmp2 = pd.DataFrame(tmp, columns={\"Predictions\"})\n",
    "tmp3 = pd.DataFrame((yTest-tmp), columns={\"Residual\"})\n",
    "resFrame = pd.concat([tmp1,tmp2,tmp3], axis=1)\n",
    "resFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy!!\n",
    "\n",
    "<h3>Statsmodels style</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statsmodels.\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model. \n",
    "X2 = sm.add_constant(xTrain)\n",
    "est = sm.OLS(yTrain, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x's are the input features in order. They are showing without their names because for the input we just used the arrays from the sklearn example above. Things like this are reasonably common when we're doing ML, as most other algorithms don't really give detailed data for each feature like linear regression does, so having them listed by names isn't super useful. To get the labels we could:\n",
    "<ul>\n",
    "<li>Feed the statsmodel a formula, like we have in the book. \n",
    "<li>Use a dataframe as an input. (Coming up)\n",
    "<li>Create a list of column names and match the up after the fact - reconstructing output that you want from the values you want.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "ypred = est2.predict(sm.add_constant(xTest))\n",
    "mean_squared_error(yTest,ypred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Density</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>Neck</th>\n",
       "      <th>Chest</th>\n",
       "      <th>Abdomen</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Thigh</th>\n",
       "      <th>Knee</th>\n",
       "      <th>Ankle</th>\n",
       "      <th>Biceps</th>\n",
       "      <th>Forearm</th>\n",
       "      <th>Wrist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0708</td>\n",
       "      <td>23</td>\n",
       "      <td>154.25</td>\n",
       "      <td>67.75</td>\n",
       "      <td>36.2</td>\n",
       "      <td>93.1</td>\n",
       "      <td>85.2</td>\n",
       "      <td>94.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>37.3</td>\n",
       "      <td>21.9</td>\n",
       "      <td>32.0</td>\n",
       "      <td>27.4</td>\n",
       "      <td>17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0853</td>\n",
       "      <td>22</td>\n",
       "      <td>173.25</td>\n",
       "      <td>72.25</td>\n",
       "      <td>38.5</td>\n",
       "      <td>93.6</td>\n",
       "      <td>83.0</td>\n",
       "      <td>98.7</td>\n",
       "      <td>58.7</td>\n",
       "      <td>37.3</td>\n",
       "      <td>23.4</td>\n",
       "      <td>30.5</td>\n",
       "      <td>28.9</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0414</td>\n",
       "      <td>22</td>\n",
       "      <td>154.00</td>\n",
       "      <td>66.25</td>\n",
       "      <td>34.0</td>\n",
       "      <td>95.8</td>\n",
       "      <td>87.9</td>\n",
       "      <td>99.2</td>\n",
       "      <td>59.6</td>\n",
       "      <td>38.9</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.8</td>\n",
       "      <td>25.2</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0751</td>\n",
       "      <td>26</td>\n",
       "      <td>184.75</td>\n",
       "      <td>72.25</td>\n",
       "      <td>37.4</td>\n",
       "      <td>101.8</td>\n",
       "      <td>86.4</td>\n",
       "      <td>101.2</td>\n",
       "      <td>60.1</td>\n",
       "      <td>37.3</td>\n",
       "      <td>22.8</td>\n",
       "      <td>32.4</td>\n",
       "      <td>29.4</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0340</td>\n",
       "      <td>24</td>\n",
       "      <td>184.25</td>\n",
       "      <td>71.25</td>\n",
       "      <td>34.4</td>\n",
       "      <td>97.3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.9</td>\n",
       "      <td>63.2</td>\n",
       "      <td>42.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Density  Age  Weight  Height  Neck  Chest  Abdomen    Hip  Thigh  Knee  \\\n",
       "0   1.0708   23  154.25   67.75  36.2   93.1     85.2   94.5   59.0  37.3   \n",
       "1   1.0853   22  173.25   72.25  38.5   93.6     83.0   98.7   58.7  37.3   \n",
       "2   1.0414   22  154.00   66.25  34.0   95.8     87.9   99.2   59.6  38.9   \n",
       "3   1.0751   26  184.75   72.25  37.4  101.8     86.4  101.2   60.1  37.3   \n",
       "4   1.0340   24  184.25   71.25  34.4   97.3    100.0  101.9   63.2  42.2   \n",
       "\n",
       "   Ankle  Biceps  Forearm  Wrist  \n",
       "0   21.9    32.0     27.4   17.1  \n",
       "1   23.4    30.5     28.9   18.2  \n",
       "2   24.0    28.8     25.2   16.6  \n",
       "3   22.8    32.4     29.4   18.2  \n",
       "4   24.0    32.2     27.7   17.7  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and remove bodyfat\n",
    "dfE = pd.read_csv(\"data/bodyfat.csv\")\n",
    "dfE.drop(columns={\"BodyFat\"}, inplace=True)\n",
    "dfE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((252, 13), (252, 1))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setup Data\n",
    "yE = np.array(dfE[\"Density\"]).reshape(-1,1)\n",
    "xE = np.array(dfE.drop(columns={\"Density\"}))\n",
    "xE.shape, yE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split and Train\n",
    "xTrainE, xTestE, yTrainE, yTestE = train_test_split(xE,yE,test_size=.3)\n",
    "#Generate model \n",
    "modelE = LinearRegression().fit(xTrainE,yTrainE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.011128385792870505\n",
      "R-squared: 0.6505217281635629\n"
     ]
    }
   ],
   "source": [
    "#Score \n",
    "#Get RMSE and R2\n",
    "tmpE = modelE.predict(xTestE)\n",
    "rmseE = mean_squared_error(tmpE, yTestE, squared=False)\n",
    "r_sqE = modelE.score(xTestE, yTestE)\n",
    "\n",
    "print(\"RMSE:\", rmseE)\n",
    "print('R-squared:', r_sqE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity - Useful, but Less Critical\n",
    "\n",
    "For fun, we'll look at the multicollinearity, because it is crazy high in this example. Look back at the pairplot, most varaibles appear highly correlated with each other - as one increases, the other does as well. This makes sense logically, as these are all measures of the body size; as your wrist size increases, so does your forearm, and your bicep, etc... They are closely correlated. One effect of this is that the impact of each variable is hard to pinpoint, as all the varaibles are largely measuring the same thing. \n",
    "\n",
    "Addressing this won't make our accuracy measures get way better, but it will allow us to better attribute the impact to the individual features, which is one advantage to a linear regression - that table of results give us some data that we can use to edit our model. \n",
    "\n",
    "We can calculate the VIF - variance inflation factor. This is a measure of multicollinearity. The calculation is VIF = 1/(1-R2), so as R2 gets closer to 1, the VIF gets larger. If we think of it in R2 terms - a big R2 indicates that a large percentage in the varaince of the results is captured in the model. Here, the R2 is how much of the variance of the other features is explained by each feature. So if they are higly correlated that value will be high and R2 will be high, leading to a big VIF value. If the R2 is low, then the varaible doesn't capture varaiance in the other features, so it is different from them, or it captures different data. In this case the R2 is small, so the VIF is small. \n",
    "\n",
    "The rule of thumb is that a VIF of ~10 should be looked at. \n",
    "\n",
    "### Variance Varies\n",
    "\n",
    "One way that makes a lot of sense to me to think of in this case, R2, and a few other places is to think of a measure of how much the data varies, and what is \"causing\" that varaince - as an intuitive sense, not a literal causitive relationship. \n",
    "\n",
    "Here, we can start with the assumption that the target varries - it literally just takes on different values. \n",
    "<ul>\n",
    "<li> If we look back to R2, that's a measure of how much of that variance is captured in the model - or put in this context, the target varries a bunch, and R2% of that variation is \"caused\" (or captured by, more accurately) the things that we have in our model, and (1-R2%) is due to other stuff - whatever our model is missing. A complete model captures most of the things that cause the data to differ - if we step back and think about it logically and not statistically, this makes a lot of sense to me personally. \n",
    "<li> If we look at ANOVA (the f-score in particular) what we are looking for is a measure of what varies more. Do we see more change (variance in the data) when comparing two groups, which is an indication that the groups are different, or do we see more change when comparing the values inside of a group (indicating the groups are more similar). \n",
    "<li> Here, we are ultimately looking to attribute the varaiance in the target to the set of features that really matters. When measuring the VIF, we are basically asking the question, \"does the data still vary just as much without this varaible?\" If we have a high VIF that tells us that this varaible doesn't really \"add any information\" - the data varies almost/just as much with it gone, so it wasn't a determinative factor in making the data change. If the VIF is low, that tells us that the amount the data changes (varies) is dependent on this varaible - it contains information that makes the data change.\n",
    "</ul>\n",
    "\n",
    "Again, this is a mental model that makes sense to me, it is not a specific calculation or rule. On the whole, in predicive analytics, we are attempting to determine \"what causes this data to vary\", so we can use those things to make a prediction. The more of that varying we capture, the better we generally do. We can also look backwards, like we are here, and attempt to attribute that varaiance we see to different \"causes\", so we can do stuff like remove the ones that aren't making that much of an impact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#Function will check the VIF of each variable in a DF and return the results in another DF\n",
    "def calc_vif(X):\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check VIFs\n",
    "calc_vif(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View the Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[[\"Hip\", \"Knee\", \"Wrist\", \"Chest\",\"Neck\",\"Thigh\",\"Ankle\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Out the Values\n",
    "\n",
    "All the scores are huge. We expected this since they all almost overlap on that pairplot earlier. What this tells us is that the information in each varaible has lots of overlap - in this case they all measure bodyfat, fairly directly. If we think about the real world impact of this it makese sense - the cheap and easy way to measure bodfat is by taking measurements and caliper pinches all over one's body. We'd expect the measurements to all be pretty correlated - it'd be a bit odd to have a huge wrist and a tiny ankle, or a big thigh and tiny knee. A few people might stand out, particularly very muscular people, but most people will be pretty consistent between every measurement based on fat level. \n",
    "\n",
    "How accurate will this model be if we drop a bunch of the high VIF stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove some high VIF columns\n",
    "df_2 = df_.drop(columns={\"Hip\", \"Knee\", \"Wrist\", \"Chest\",\"Neck\",\"Thigh\",\"Ankle\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run statsmodesl to get results\n",
    "X2 = sm.add_constant(df_2)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recheck\n",
    "calc_vif(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE for smaller model \n",
    "x2 = np.array(df_2)\n",
    "x2.shape\n",
    "model2 = LinearRegression().fit(x2,y)\n",
    "ypred2 = model2.predict(x2)\n",
    "mean_squared_error(y,ypred2,squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Smaller Model Results</h3>\n",
    "\n",
    "Removing variables doesn't up the R2 - we'll explore that more later on in ML.\n",
    "\n",
    "The RMSE doesn't get much worse (and the change is within the expectation of randomness here) - that makes sense because what we found with this multicollinearity was that each varaible contained the same information. So when we remove one duplicate piece of that information, not much changes. This process is called feature selection, it is one of the big things in ML, and we'll look at it in different ways as we do next semester. \n",
    "\n",
    "When we are just using CSVs full of data, there isn't a very large impact in removing a few variables like this. If we were dealing with data that we had to collect - e.g. if we were paying nurses to do all of these measurements on people - then we'd want to minimize the number of features needed, so our nurses could be faster and therefore cheaper. Similarly, if we had massive amounts of data and training our model or storing the data was costly, being able to be accurate with less data is good. We don't often run into constraints in learning, because the data is small, but they can exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can cut it down to the bare-bones - I'll take the vars with small p values\n",
    "df_3 = df[[\"Abdomen\",\"Weight\"]]\n",
    "X2 = sm.add_constant(df_3)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE for tiny model \n",
    "x2 = np.array(df_3)\n",
    "x2.shape\n",
    "model2 = LinearRegression().fit(x2,y)\n",
    "ypred2 = model2.predict(x2)\n",
    "mean_squared_error(y,ypred2,squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the moral of the story? We can predict almost as well with just two varaibles as we did with all of them!\n",
    "\n",
    "Challenge - run this with a split of the data. One part to train, another to test. Not the accuracy differences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run above with test/train split"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea39297c2a3b8433e0e3c4b620aff79df88eb4bda961dfb2311fbafd7efdbd77"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
