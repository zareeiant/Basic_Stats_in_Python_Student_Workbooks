{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinkplot\n",
    "import thinkstats2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from fractions import Fraction\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes and Updates\n",
    "\n",
    "We will solve the Monty Hall problem, but first we can work on some more simple examples and build out a method for using Bayes probability calculations. Remember the Bayes theorem from before:\n",
    "\n",
    "$ P(A|B) = \\frac{P(A) P(B|A)}{P(B)} $\n",
    "\n",
    "Suppose you have two high school classes of 40 students - class A and class B. Each class has some failing students:\n",
    "<ul>\n",
    "<li> Class A has 10 failing students, 30 passing ones. \n",
    "<li> Class B has 20 failing students, 20 passing ones.\n",
    "</ul>\n",
    "\n",
    "<b>If we randomly select one failing student, what is the probability they are from Class A?</b>\n",
    "\n",
    "We can calculate this out as a table. First, another way to think of Bayes...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diachronic Bayes\n",
    "\n",
    "There is another way to think of Bayes's theorem: it gives us a way to\n",
    "update the probability of a hypothesis, $H$, given some body of data, $D$.\n",
    "\n",
    "This interpretation is \"diachronic\", which means \"related to change over time\"; in this case, the probability of the hypotheses changes as we see new data.\n",
    "\n",
    "Rewriting Bayes's theorem with $H$ and $D$ yields:\n",
    "\n",
    "$ P(H|D) = \\frac{P(H)~P(D|H)}{P(D)} $\n",
    "\n",
    "In this interpretation, each term has a name:\n",
    "\n",
    "-  $P(H)$ is the probability of the hypothesis before we see the data, called the prior probability, or just **prior**.\n",
    "\n",
    "-  $P(H|D)$ is the probability of the hypothesis after we see the data, called the **posterior**.\n",
    "\n",
    "-  $P(D|H)$ is the probability of the data under the hypothesis, called the **likelihood**.\n",
    "\n",
    "-  $P(D)$ is the **total probability of the data**, under any hypothesis.\n",
    "\n",
    "Sometimes we can compute the prior based on background information. For example, the classroom problem specifies that we choose a student at random with equal probability.\n",
    "\n",
    "In other cases the prior is subjective; that is, reasonable people might disagree, either because they use different background information or because they interpret the same information differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Passing and Failing\n",
    "\n",
    "From above:\n",
    "<ul>\n",
    "<li> Class A has 10 failing students, 30 passing ones. \n",
    "<li> Class B has 20 failing students, 20 passing ones.\n",
    "</ul>\n",
    "\n",
    "First, we can build a table to hose our work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(index=['Class A', 'Class B'])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Prior Probability\n",
    "\n",
    "We can then add in the first part - the prior probability. \n",
    "\n",
    "We can also think of this as \"the probability before we know anything else\" - here we are just finding the probability of them being in Class A, without adding any other information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['prior'] = 1/2, 1/2\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Likelihoods\n",
    "\n",
    "Add the likelihoods in...\n",
    "For class A there is a 25% chance of a student failing.\n",
    "For class B, it is 50%\n",
    "\n",
    "We can also think of the likelihoods explicitly as conditional statements:\n",
    "<ul>\n",
    "<li> E.g. \"If I choose from class B, what is the likelihood of getting a failiure?\"\n",
    "<li> Or, give the prior is true, now what is the probability?\n",
    "</ul>\n",
    "\n",
    "You are assuming the \"Question part\" of the original goal - what are the chances this class provides a failiure, given the stipulation of the prior probability. \n",
    "\n",
    "<ul>\n",
    "<li> If I choose A, 10 out of 40 are failing, so the chances are 1/4\n",
    "<li> If I choose B, 20 out of 40 are failing, so the chances are 1/2\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['likelihood'] = 1/4, 1/2\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Interim Probabilities\n",
    "\n",
    "Next, multiply the two probabilities together:\n",
    "<ul>\n",
    "<li>E.g. There's a 50% chance of choosing class B, and if I do, there's a 50% chance of getting a fail. \n",
    "</ul>\n",
    "We label this column the unnorm - or unnormalized probabilities. This is because they are both accurate probabilities, but they are not normalized - they do not sum to 1. If we look a little closer, they are also part our boy Bayes' Theorem:\n",
    "<ul>\n",
    "<li>The numerator of Bayes is a probability multiplied by a conditional, which is the unnorm value.\n",
    "<li>The denomenator is the total probability - there are only 2 cases here, one must be true, so it is the sum of the unnorms.\n",
    "</ul>\n",
    "\n",
    "We can also think of these probabilities as being \"in terms of\" the total possible outcomes - i.e. 12.5 of <i>all</i> students are failing in Class A, while 25% of students is failing in B. The other 62.5% are passing, in either class, so they aren't really a consideration in the calculation we are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate unnormalized probabilities\n",
    "table['unnorm'] = table['prior'] * table['likelihood']\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, we can demonstrate that last point is true:\n",
    "\n",
    "- Calculate the total probability of getting a fail by summing the unnorms.\n",
    "\n",
    "- Calculate the total probability of getting a fail by direct calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_data = table['unnorm'].sum()\n",
    "print(\"Unnorms:\", prob_data)\n",
    "probDirect = (10+20)/(40+40) #The overall fail chances - 30 total failiures, 80 total students. \n",
    "print(\"Direct:\", probDirect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Posterior Probabilities\n",
    "\n",
    "Now we can normalize - or make the probs total to 1. We just divide by that total probability. This gives us the posterior probabilities, answering our original question:\n",
    "\n",
    "$ P(Class A | Failing) $\n",
    "\n",
    "As well as giving us the other probabilites, for free. \n",
    "\n",
    "This step just shifts that probability that we calculated above to be \"out of\" the total we care about (failing people), rather than the entire total of all students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['posterior'] = table['unnorm'] / prob_data\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Update Formula\n",
    "\n",
    "We can wrap those last steps un into a formula, since they are just calculating from the probabilites that we've provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(table):\n",
    "    \"\"\"Compute the posterior probabilities.\"\"\"\n",
    "    table['unnorm'] = table['prior'] * table['likelihood']\n",
    "    prob_data = table['unnorm'].sum()\n",
    "    table['posterior'] = table['unnorm'] / prob_data\n",
    "    return prob_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice problem\n",
    "\n",
    "Suppose I have a box with a 6-sided die, an 8-sided die, and a 12-sided die. I choose one of the dice at random, roll it, and report that the outcome is a 1. What is the probability that I chose the 6-sided die?\n",
    "\n",
    "<b>Note:</b> The fractions function will give us fractions rather than decimals. It is helpful here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the table\n",
    "dice = pd.DataFrame(index=[\"Six Side\", \"Eight Side\", \"12 Side\"])\n",
    "dice[\"prior\"] = Fraction(1,3)\n",
    "dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The probability for each die being a 1, given I pick it.\n",
    "dice[\"likelihood\"] = Fraction(1, 6), Fraction(1, 8), Fraction(1, 12)\n",
    "dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update to finish\n",
    "update(dice)\n",
    "dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Scenario\n",
    "\n",
    "Suppose you are placing a sports bet on your favorite team - The Bayes. You know a few things:\n",
    "<ul>\n",
    "<li>The Bayes have a 50% chance of winning a game. (Based on past performance)\n",
    "<li>The Bayes have had a 10% chance of having rain in their games in Bayes Stadium.\n",
    "<li>However, in games that The Bayes have won in Bayes Statium, there's be a 11% chance of rain. \n",
    "<li>So...\n",
    "<li>P(W) = 50%\n",
    "<li>P(R) = 10%\n",
    "<li>P(R|W) = 11%\n",
    "</ul>\n",
    "\n",
    "<b>What is the probability that The Bayes win if it rains? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Feature Bayes\n",
    "\n",
    "What is the probability of playing when the weather is sunny, and the temperature is cool. To do so we'll break it into two steps. Each time we do an update on our table, that's incorporating one new piece of information. Now, we can do it more than once, each one will be basically a \"redo\" of the process. \n",
    "\n",
    "We can think of this as updating our understanding, one varaible at a time. We start with the prior probability, then proceed to \"add\" knowledge to our understanding, one feature at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw = pd.read_csv(\"data/weather.txt\", sep=\"\\t\")\n",
    "dfw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables\n",
    "\n",
    "We want to predict if we are going to play, so we will setup our inital table with those two options and their prior probabilities. \n",
    "\n",
    "<b>Note:</b> I've also calculated some other totals that we'll need later here. We could also calculate them later, as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.DataFrame(index=[\"Play\", \"Not Play\"])\n",
    "\n",
    "# Other calculations for later\n",
    "# You can mostly ignore this for the moment\n",
    "total = len(dfw)\n",
    "dfPlay = dfw[dfw[\"Play\"] == \"yes\"]\n",
    "dfNoPlay = dfw[dfw[\"Play\"] == \"no\"]\n",
    "playTotal = Fraction(len(dfPlay))\n",
    "pOutlook = Fraction(len(dfw[dfw[\"Outlook\"] == \"Sunny\"]), total)\n",
    "pTemp = Fraction(len(dfw[dfw[\"Temp\"] == \"Cool\"]), total)\n",
    "# end other stuff\n",
    "\n",
    "pPlay = Fraction(len(dfw[dfw[\"Play\"] == \"yes\"]), total)\n",
    "weather[\"prior\"] = pPlay, (1 - pPlay)\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Outlook\n",
    "\n",
    "Our first update will add the information for the Outlook. This step isn't really any different than before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_playOut = Fraction(len(dfPlay[dfPlay[\"Outlook\"] == \"Sunny\"]), len(dfPlay))\n",
    "if_notOut = Fraction(len(dfNoPlay[dfNoPlay[\"Outlook\"] == \"Sunny\"]), len(dfNoPlay))\n",
    "weather[\"likelihood\"] = if_playOut, if_notOut\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(weather)\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors with Multiple Variables\n",
    "\n",
    "Now we'll do the second round of our updates. Here, we'll need to \"grab\" the result of the previous update as our starting point has changed. Now we aren't starting with the simple intial probabilities, we are starting having already \"learned\" of the Outlook information. \n",
    "\n",
    "#### Add Temp\n",
    "\n",
    "We will take the outcome of the previous table as our priors. \n",
    "\n",
    "<b>Thought Experiment:</b> in the example below there are both the normalized and unnormalized probabilities. Try using either for the prior probs, what happens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = weather[[\"posterior\"]]\n",
    "w2 = w2.rename(columns={\"posterior\": \"prior\"})\n",
    "#w2 = weather[[\"unnorm\"]]\n",
    "#w2 = w2.rename(columns={\"unnorm\": \"prior\"})\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_playTemp = Fraction(len(dfPlay[dfPlay[\"Temp\"] == \"Cool\"]), len(dfPlay))\n",
    "if_notTemp = Fraction(len(dfNoPlay[dfNoPlay[\"Temp\"] == \"Cool\"]), len(dfNoPlay))\n",
    "w2[\"likelihood\"] = if_playTemp, if_notTemp\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(w2)\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "We would predict this as a \"play\", since we are about 71% likely to think that is a play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Calculation of Multiple Varaible Bayes\n",
    "\n",
    "We can also calculate this a bit more directly. The Bayes formula can be expanded into dealing with multiple features. \n",
    "\n",
    "![Naive Bayes](images/naive_bayes.png \"Naive Bayes\")\n",
    "\n",
    "What do we have? The conditional probability, when conditioned on multiple values, is:\n",
    "<ul>\n",
    "<li> The product of all the \"flipped\" individual conditional probabilities, multiplied by the overall probability. Divided by...\n",
    "<li> The product of the probabilities of the varaibles themselves.\n",
    "</ul>\n",
    "\n",
    "So for us it is:\n",
    "\n",
    "$ P(Golf | Sunny+Cool) = \\frac{P(Sunny | Golf) * P(Cool | Golf) * P(Golf)}{P(Sunny) * P(Cool)} $\n",
    "\n",
    "If we bust out the math..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditionals - Probabilities if we golf\n",
    "pOutlookPlay = len(dfPlay[dfPlay[\"Outlook\"] == \"Sunny\"]) / playTotal\n",
    "pTempPlay = len(dfPlay[dfPlay[\"Temp\"] == \"Cool\"]) / playTotal\n",
    "\n",
    "# Negatives - Probabilites if we don't\n",
    "pOutlookNo = len(dfNoPlay[dfNoPlay[\"Outlook\"] == \"Sunny\"]) / len(dfNoPlay)\n",
    "pTempNo = len(dfNoPlay[dfNoPlay[\"Temp\"] == \"Cool\"])/ len(dfNoPlay)\n",
    "\n",
    "#Denominator (this doesn't change)\n",
    "pDen = float(pOutlook * pTemp)\n",
    "\n",
    "# Raw Likelihoods\n",
    "like_play = float(pOutlookPlay * pTempPlay * pPlay)\n",
    "like_noplay = float(pOutlookNo * pTempNo * (1-pPlay))\n",
    "\n",
    "print(like_play, like_noplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can normalize the likelihoods and get reall probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "nPlay = like_play / pDen\n",
    "nNot = like_noplay / pDen\n",
    "tot_prob = nPlay + nNot\n",
    "\n",
    "print((nPlay/tot_prob), (nNot/tot_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Looks pretty similar. We are awesome. \n",
    "\n",
    "### Likelihoods\n",
    "\n",
    "One note when using Bayes as a classifier. All we really care about here is the likelihoods, not the final probability. We are going to end up making a prediction for whatever has the higher likelihood, since the denominators are the same for each probability calculation. The final normalization part can kind of be ingored, it makes the result more readable, but doesn't actually impact what we will do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Example - Sunny, Hot, Normal, False\n",
    "\n",
    "We can generate a \"full\" prediction as well - we'll use all 4 features to make a prediciton. \n",
    "\n",
    "We start the same way as always - setup the prior probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherBig = pd.DataFrame(index=[\"Play\", \"Not Play\"])\n",
    "weatherBig[\"prior\"] = pPlay, (1 - pPlay)\n",
    "weatherBig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outlook\n",
    "\n",
    "We will update the probabilities with the Outlook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_playOut2 = Fraction(len(dfPlay[dfPlay[\"Outlook\"] == \"Sunny\"]), len(dfPlay))\n",
    "if_notOut2 = Fraction(len(dfNoPlay[dfNoPlay[\"Outlook\"] == \"Sunny\"]), len(dfNoPlay))\n",
    "print(Fraction(if_playOut2), Fraction(if_notOut2))\n",
    "\n",
    "# Ignore - this is to check answers against a solution with slightly different data\n",
    "#if_playOut2 = Fraction(2,9)\n",
    "#if_notOut2 = Fraction(3,5)\n",
    "\n",
    "weatherBig[\"likelihood\"] = if_playOut2, if_notOut2\n",
    "update(weatherBig)\n",
    "weatherBig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temperature\n",
    "\n",
    "Take the existing probabilities as the priors, do another update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wB1 = weatherBig[[\"unnorm\"]]\n",
    "wB1 = wB1.rename(columns={\"unnorm\": \"prior\"})\n",
    "wB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_playTemp2 = Fraction(len(dfPlay[dfPlay[\"Temp\"] == \"Hot\"]), len(dfPlay))\n",
    "if_notTemp2 = Fraction(len(dfNoPlay[dfNoPlay[\"Temp\"] == \"Hot\"]), len(dfNoPlay))\n",
    "print(Fraction(if_playTemp2), Fraction(if_notTemp2))\n",
    "wB1[\"likelihood\"] = if_playTemp2, if_notTemp2\n",
    "update(wB1)\n",
    "wB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Humidity\n",
    "\n",
    "Take the existing probabilities as the priors, do another update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wB2 = wB1[[\"unnorm\"]]\n",
    "wB2 = wB2.rename(columns={\"unnorm\": \"prior\"})\n",
    "wB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_playHum2 = Fraction(len(dfPlay[dfPlay[\"Humidity\"] == \"Normal\"]) , len(dfPlay))\n",
    "if_notHum2 = Fraction(len(dfNoPlay[dfNoPlay[\"Humidity\"] == \"Normal\"]) , len(dfNoPlay))\n",
    "print(Fraction(if_playHum2), Fraction(if_notHum2))\n",
    "wB2[\"likelihood\"] = if_playHum2, if_notHum2\n",
    "update(wB2)\n",
    "wB2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wind\n",
    "\n",
    "Take the existing probabilities as the priors, do another update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wB3 = wB2[[\"unnorm\"]]\n",
    "wB3 = wB3.rename(columns={\"unnorm\": \"prior\"})\n",
    "wB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_playWind2 = Fraction(len(dfPlay[dfPlay[\"Windy\"] == \"f\"]) , len(dfPlay))\n",
    "if_notWind2 = Fraction(len(dfNoPlay[dfNoPlay[\"Windy\"] == \"f\"]) , len(dfNoPlay))\n",
    "print(Fraction(if_playWind2), Fraction(if_notWind2))\n",
    "\n",
    "wB3[\"likelihood\"] = if_playWind2, if_notWind2\n",
    "update(wB3)\n",
    "wB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "125/152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Direct Calculation\n",
    "\n",
    "We can use the formula above - calculate each likelihood then normalize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditionals\n",
    "pOutlook = Fraction(len(dfw[dfw[\"Outlook\"] == \"Sunny\"]), total)\n",
    "pTemp = Fraction(len(dfw[dfw[\"Temp\"] == \"Hot\"]), total)\n",
    "pHum = Fraction(len(dfw[dfw[\"Humidity\"] == \"Normal\"]), total)\n",
    "pWind = Fraction(len(dfw[dfw[\"Windy\"] == \"f\"]), total)\n",
    "\n",
    "big_num = pPlay * if_playOut2 * if_playTemp2 * if_playHum2 * if_playWind2\n",
    "big_not = (1-pPlay) * if_notOut2 * if_notTemp2 * if_notHum2 * if_notWind2\n",
    "big_den = pOutlook * pTemp * pHum * pWind\n",
    "\n",
    "# Normalize and show final probs\n",
    "play_prob = float(big_num/big_den)\n",
    "not_prob = float(big_not/big_den)\n",
    "tot_prob = play_prob + not_prob\n",
    "print(\"Yes\",  big_num/big_den, play_prob, play_prob/tot_prob)\n",
    "print(\"No\",  big_not/big_den, not_prob, not_prob/tot_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Looking Forward\n",
    "\n",
    "Going through this should set off a few bells in your mind - we are using a bunch of features to generate a prediction....\n",
    "\n",
    "As you may guess, Bayes is the basis of a (set of) predictive model. Right now we are doing a version of Naive Bayes, which is a common simple classification model, often used for things like spam detection, because it is very fast. \n",
    "\n",
    "Next time we'll build these concepts up into a full blown predictive model algorithm, from scratch! \n",
    "\n",
    "![Bayes](images/bayes.jpeg \"Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Cars\n",
    "\n",
    "What if we want to know the odds that a car is stolen given it is a BMW, black, and at night? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_car = pd.read_csv(\"data/vehicle_stolen_dataset.csv\", names=[\"ID\", \"Make\", \"Color\", \"Time\", \"Stolen\"])\n",
    "df_car.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thought Intermission - The Monty Hall Problem\n",
    "\n",
    "Next we'll use a Bayes table to solve one of the most contentious problems in probability.\n",
    "\n",
    "The Monty Hall problem is based on a game show called *Let's Make a Deal*. If you are a contestant on the show, here's how the game works:\n",
    "<ul>\n",
    "<li> The host, Monty Hall, shows you three closed doors -- numbered 1, 2, and 3 -- and tells you that there is a prize behind each door.\n",
    "<li> One prize is valuable (traditionally a car), the other two are less valuable (traditionally goats).\n",
    "<li> The object of the game is to guess which door has the car. If you guess right, you get to keep the car.\n",
    "</ul>\n",
    "\n",
    "The key - after you pick a door, Monty will open another, revealing a goat. Then Monty offers you the option to stick with your original choice or switch to the remaining unopened door. To maximize your chance of winning the car, should you stick with Door 1 or switch to Door 2?\n",
    "\n",
    "To answer this question, we have to make some assumptions about the behavior of the host (these are parts of the general rules of the game):\n",
    "<ul>\n",
    "<li> Monty always opens a door and offers you the option to switch.\n",
    "<li> He never opens the door you picked or the door with the car.\n",
    "<li> If you choose the door with the car, he chooses one of the other doors at random.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start off - initially the chances are equal for each door. \n",
    "#So the prior probabilities are all 1/3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to decide what door we want - let's get that whip. \n",
    "\n",
    "<b>We'll assume we pick door 1.</b>\n",
    "\n",
    "Then Monty opens one of the other doors, to us it is random. When he does so it gives us the likelihoods.\n",
    "\n",
    "<b>We'll assume he opens door 3 - remember he always opens a goat door, not the car</b>\n",
    "\n",
    "Now, that we know that it isn't door 3 (that's open, it is a goat). Remember, each one is a hypothetical - if we are in this \"class\" (door choice), what is the probability of \"success\" (a car there)?: \n",
    "\n",
    "\n",
    "We can think about this by carefully defining the problem - What are the odds that Monty opened Door 3, given that the Car is in Door X:\n",
    "<ul>\n",
    "<li>The likelihood he'd open Door 3 if the car is there is 0 - we can see the goat, and that's the rules. \n",
    "<li>The likelihood he'd open Door 3 if the car is in Door 2 is 1 - he'd be forced to by the rules of the game, you picked Door 1, Door 2 has the car, so he can only open Door 3.\n",
    "<li>The linkelihood he'd open Door 3 if the car is in Door 1 is 1/2 - he just picks randomly \n",
    "</ul>\n",
    "\n",
    "key - we don't really know the probability the car is in Door X directly. We can use the probability that the Door is opened, and the rules of the game to calculate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can update the table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing it mathmatically requires a bunch of derivation: https://en.wikipedia.org/wiki/Monty_Hall_problem\n",
    "\n",
    "Alternate explaination that I think is the most clear way to imagine it: Initially there is a 1/3 chance of the car being behind each door. However, after you choose those odds change, due to the rules of the game:\n",
    "<ul>\n",
    "<li>The chances it is in your door is still 1/3.\n",
    "<li>The chances it is not in your door is 2/3.\n",
    "<li>The door opening part sets the odds for one door to 0, so that 2/3 is contained entirely in one door. \n",
    "</ul>\n",
    "\n",
    "The entire point of this problem is to be unituitive, so having it be confusing is normal. \n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
