{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Stuff\n",
    "import thinkplot\n",
    "import thinkstats2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Density Functions\n",
    "\n",
    "Probability density functions (PDFs) are another of our commonly used visualizations to show and explore data. \n",
    "\n",
    "Going forward, the PDF is probably the most generally useful of the assorted distribution graphs, especially with continuous data. All of them do the same thing, but as we've seen there are some scenarios where the histogram looks weird and isn't useful. The PDF will pretty much always give us a good idea of the distribution. \n",
    "\n",
    "## Discreet and Continuous\n",
    "\n",
    "The relationship between the discreet and continuous distributions is important because we sometimes need/want to transform our data between the two. \n",
    "\n",
    "One example from real life is your GPA - when you do assignments/exams, you end up with a raw percentage grade which is continuous. When this is converted to a letter scale (A, B, etc...), that letter scale is discreet - there's only a selection of possible values (b-,b,b+, etc...). This is binning. We take a continuous varaible and create a discreet variable from it - or more literally, we put the continuous values into a bin. \n",
    "\n",
    "The other example is when your GPA is caclulated - those discreet values are assigned numbers on a 1-4 scale, then averaged together creating a new continuous value - your GPA.\n",
    "\n",
    "One place where this is commonly used is lending and credit scores. Having a credit score of 752 vs 764 makes no difference, you're placed in a category of \"excellent\", \"very good\", etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_csv(\"data/loan_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Winning is Binning\n",
    "\n",
    "Below is an example of how binning works that you can walk through. The logic is fairly simple, we see what bucket our numerical value falls into, then give it a label for that group. Here we are giving credit scores their label as though someone was applying for credit. \n",
    "\n",
    "<b>Note:</b> I have commented out some of my troubleshooting print statements, try uncommenting and executing the loop, and take a look at what gets printed. If your loop was not delivering what you want, would this be useful in finding the issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create credit-score bucket.\n",
    "df[\"grade\"] = \" \"\n",
    "scoreCol = df.columns.get_loc(\"fico\")\n",
    "gradeCol = df.columns.get_loc(\"grade\")\n",
    "\n",
    "for i in range(len(df)) :\n",
    "    if df.iloc[i,scoreCol] < 580:\n",
    "        #print(\"Less than 580-\"+str(df.iloc[i,scoreCol]))\n",
    "        df.iloc[i,gradeCol] = \"subprime\"\n",
    "    elif df.iloc[i,scoreCol] < 670:\n",
    "        #print(\"580-670-\"+str(df.iloc[i,scoreCol]))\n",
    "        df.iloc[i,gradeCol] = \"fair\"\n",
    "    elif df.iloc[i,scoreCol] < 740:\n",
    "        #print(\"670-740-\"+str(df.iloc[i,scoreCol]))\n",
    "        df.iloc[i,gradeCol] = \"good\"\n",
    "    elif df.iloc[i,scoreCol] < 800:\n",
    "        #print(\"740-800-\"+str(df.iloc[i,scoreCol]))\n",
    "        df.iloc[i,gradeCol] = \"very good\"\n",
    "    else:\n",
    "        #print(\"800+-\"+str(df.iloc[i,scoreCol]))\n",
    "        df.iloc[i,gradeCol] = \"excellent\"\n",
    "#print(str(scoreCol)+ \" \"+ str(gradeCol))\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual Binning via a Loop - Details\n",
    "\n",
    "What we have above is an example of using a simple-ish loop, let's look at the parts in detail:\n",
    "<ul>\n",
    "<li> The \"for\" part says, \"do what's in this loop once for every index (the position in the dataframe - row 1, 2, 3, 4, etc). Each time the stuff is executed, make the variable 'i' be the row number\". \n",
    "<li> The series of if statements checks if the fico score value is less than a series of cutoffs. If the fico score meets any of the criteria, it gets that label, the current execution of the loop ends, and we progress to the next row. \n",
    "<li> The iloc part is not the general way we want to use a dataframe, it is grabbing data from the df based on the index values - the row and column numbers. The 'i' is the row number, it starts at 0 and increases with each execution of the loop. The scoreCol value is calculated above, it is just the column number of the \"fico\" column (6 for me). The whole statement effectively grabs the \"cell\" at that location, like an Excel cell or a battleship hit. \n",
    "<li> This repeats, with 'i' incrementing until we reach the end. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built in Binning\n",
    "\n",
    "In reality, we can use premade functions to do the work for us. Here we can define the cutoff limits and let the computer figure our the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In generic cases, we can automate this:\n",
    "bins = np.arange(580, 860, 60) #or\n",
    "bins = np.array([580, 670, 740, 800])\n",
    "indicies = np.digitize(df[\"fico\"], bins)\n",
    "groups = df.groupby(indicies)\n",
    "for i, group in groups:\n",
    "    print(i, group[\"fico\"].min(), len(group), np.exp(group[\"log.annual.inc\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph\n",
    "#hist2 = thinkstats2.Hist(round(df[\"fico\"], -1))\n",
    "hist2 = thinkstats2.Hist(df[\"fico\"])\n",
    "pmf2 = thinkstats2.Pmf(df[\"fico\"])\n",
    "cdf2 = thinkstats2.Cdf(df[\"fico\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create graphs\n",
    "thinkplot.PrePlot(6, rows =2, cols=3)\n",
    "thinkplot.Hist(hist2)\n",
    "thinkplot.SubPlot(2)\n",
    "thinkplot.Pmf(pmf2)\n",
    "thinkplot.SubPlot(3)\n",
    "thinkplot.Cdf(cdf2)\n",
    "thinkplot.SubPlot(4)\n",
    "thinkstats2.NormalProbabilityPlot(df[\"fico\"])\n",
    "thinkplot.SubPlot(5)\n",
    "thinkstats2.NormalProbabilityPlot(np.log(df[\"fico\"]))\n",
    "thinkplot.SubPlot(6)\n",
    "pdf = thinkstats2.EstimatedPdf(df[\"fico\"]) #See more below\n",
    "thinkplot.Pdf(pdf)\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Plots on a Grid\n",
    "\n",
    "In the examples both above and below we have a grid of multiple graphs. Each one is done in a different way. \n",
    "\n",
    "In the thinkplot stuff we have a helper called PrePlot, which we can use to define the size of the grid, along with the subPlot function that can basically \"aim\" the next chart into the desired slot. \n",
    "\n",
    "In the seaborn stuff below, we have a grid using pyplot subplots. Here we define a set of sublots, then in each individual plot we assign it a slot. \n",
    "\n",
    "Both of these are built upon the foundations of pyplot (which is part of matplotlib), so there is often, but not always interoperability. The method below is farily generic and will work in many places, with many graphs. Note that (for reasons we really don't care about, it has to do with if the function only provides an \"image\" of a graph, or an entire \"object\" of a graph) there are some scenarios that making a grid like this won't work. The easiest way to confirm is to Google, \"seaborn [graphtype] subplots\" and there will be several examples (usually stackoverflow ones have good explained examples) that can normally be easily adapted. There are a bunch of ways to do this, and a bunch of semi-related garphing libraries and types of graphs, so there isn't a \"one true answer\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use seaborn as well\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5) #makes the default size larger. \n",
    "#Everything after the comma is optional. \n",
    "fig, ax = plt.subplots(1,2)\n",
    "sns.distplot(df[\"fico\"], kde_kws={\"color\":\"red\", \"label\":\"KDE\"}, hist_kws={\"label\":\"Data\"}, ax=ax[1])\n",
    "sns.distplot(df[\"fico\"], bins=bins, kde_kws={\"color\":\"red\", \"label\":\"KDE\"}, hist_kws={\"label\":\"Data\"}, ax=ax[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE and Histogram\n",
    "\n",
    "As above - the KDE produces a smoothed function, and approximates the distribution of the histogram. Especially when the volume of data becomes large, a KDE may be a more useful visualization of a distribution. \n",
    "\n",
    "The smaller those bins get, the closer of an approximation. The smoothing factor accounts for 'noise' - e.g. around 750ish. On the whole, this delivers a distribution shape that isn't impacted directly by sharp ups and downs at certain points on the distribution, like a histogram is. For generalizing from a sample to a population, this is useful...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness\n",
    "\n",
    "![Skew](images/skew.png)\n",
    "\n",
    "Skewness. We can visually see the skew - this one is right skewed a bit - the right side is \"stretched\" out a bit more. Skew is both easy to see and easy to calculate, we also have a quick rule of thumb that we can figure out with basic satistics:\n",
    "<ul>\n",
    "<li>If the mean is greater than the median, the distribution is positively skewed.\n",
    "<li>If the mean is less than the median, the distribution is negatively skewed.\n",
    "</ul>\n",
    "\n",
    "The value of the skew can be calculated with a function called Pearson Median Skewness, which is defined as:\n",
    "\n",
    "$$ Skew = \\frac{3(X_i - \\mu)}{\\sigma} $$\n",
    "\n",
    "The values for skew don't have any specific meaning, the larger magnitude it is, the larger the skew. As a rule of thumb, values greater than 1 are highly skewed. \n",
    "\n",
    "We can verify with caclculations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skew\n",
    "skw = thinkstats2.PearsonMedianSkewness(df[\"fico\"])\n",
    "print(df[\"fico\"].mean())\n",
    "print(df[\"fico\"].median())\n",
    "print(skw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show it a little more clearly on the graph by adding some reference lines for mean and median. We can see that the mean is now greater than the median. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinkplot.PrePlot(1)\n",
    "thinkplot.Pdf(pdf)\n",
    "thinkplot.axvline(df[\"fico\"].mean(), color=\"Red\", label=\"Mean\")\n",
    "thinkplot.axvline(df[\"fico\"].median(), color=\"Green\", label=\"Median\")\n",
    "thinkplot.Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income\n",
    "\n",
    "We can explore a different varaible similarly - income. \n",
    "\n",
    "We are given the income in log format. Why might that be? Can you investigate a little, and add normal income to the dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column - income. This should show the regular income, not log transformed. \n",
    "df[\"income\"] = np.exp(df[\"log.annual.inc\"])\n",
    "df[\"income\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1 - Create a Function\n",
    "\n",
    "Try to make a function that prints all the 6 graphs we made above when provided with data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge - try to create a function that makes the suite of 6 graphs above.\n",
    "# Note: you can print things in the function, this one doesn't need to return anything. \n",
    "# Note 2: this function shouldn't reference anything outside of its arguments, and imported libraries. \n",
    "#         One of the big things that makes functions useful is that we can use the function in other programs\n",
    "#         and it won't \"need\" anything else to function. Like how we can use np.mean(data) with anyhing in the data part. \n",
    "def bigGraph(df_in, columnName):\n",
    "\n",
    "    # Fill Me!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2: Establish Tax Rates\n",
    "\n",
    "Try to use the data - break into groups of marginal tax rates:\n",
    "<ul>\n",
    "<li>15% on the first $49,020 of taxable income, plus\n",
    "<li>20.5% on the next $49,020 of taxable income (on the portion of taxable income over 49,020 up to $98,040), plus\n",
    "<li>26% on the next $53,939 of taxable income (on the portion of taxable income over $98,040 up to $151,978), plus\n",
    "<li>29% on the next $64,533 of taxable income (on the portion of taxable income over 151,978 up to $216,511), plus\n",
    "<li>33% of taxable income over $216,511\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generate bins for each tax bracket</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bins for each tax bracket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the 6 graph set of graphs for original log income\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Use the data to estimate the number of people in each tax bracket</b>\n",
    "\n",
    "Try to look through the bins, if you can. See the solution for a hint, if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count numbers in each bracket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Use the cdf to estimate the number of people who earn Teacher Money - lowest: 59,357, highest: 101,162</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate number of people\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create a PDF showing the distributiion of income. Try both log income, and raw income. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #2: Tax Function\n",
    "\n",
    "Try this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Challenge - Create a function that takes an income and returns a tax bill, and marginal tax rate:\n",
    "def muhTaxes(income):\n",
    "    \n",
    "    return taxbill, margRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When calling that function, you can do something like this to get the results:\n",
    "# tax_bill, marginal_rate = muhTaxes(amount_i_earned)\n",
    "# print(\"Tax bill: \", tax_bill)\n",
    "# print(\"Marginal rate:\", marginal_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So... Why a Log?\n",
    "\n",
    "Think quickly - we have both the log and non-log data (there's just a math operation that differs between them, so we can always translate - or transform - back and forth), why bother with the log? \n",
    "\n",
    "We are ultimately concerned with using our data to draw inference, which means we are introducing some assumptions. Suppose we want to use the data to estimate the distance between the lowest and highest quintiles of income. If we only care about the sample's data, we can just count, no transformation needed. If we are attempting to generalize from our sample to the population (as we will soon), using the normal distribution allows us to use all of the things that are built to deal with normal distributions - in this case slicing segments of the data. Similarly, when we want to do tests to measure things about our data (such as hypothesis tests), many of those tests are created for normal data specifically. \n",
    "\n",
    "In general, we want to make data \"as easy as possible to analyze\", and making data normal (or sometimes transforming it in different ways, like normalization), often makes it easier. So we do it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
